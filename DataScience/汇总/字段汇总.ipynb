{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 读取主数据集...\n",
      "主数据集包含 351 个项目\n",
      "\n",
      "🔄 开始添加字段数据...\n",
      "============================================================\n",
      "\n",
      "📁 处理文件夹: 更多项目信息\n",
      "📄 目标文件: 更多项目信息_提取结果.csv\n",
      "🎯 添加字段: ['更多项目信息']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['更多项目信息']\n",
      "     📝 添加: 更多项目信息\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 更多项目信息: 343/351\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 课程网址\n",
      "📄 目标文件: 课程网址_规则处理结果.csv\n",
      "🎯 添加字段: ['课程网址']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['课程网址']\n",
      "     📝 添加: 课程网址\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 课程网址: 160/351\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 面试\n",
      "📄 目标文件: 面试需求_处理结果.csv\n",
      "🎯 添加字段: ['面试newp']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['面试newp']\n",
      "     📝 添加: 面试newp\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 面试newp: 298/351\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 申请费减免\n",
      "📄 目标文件: final_申请费减免_gemini-2.5-flash_median_accuracy_processed.csv\n",
      "🎯 添加字段: ['申请费减免']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['申请费减免']\n",
      "     📝 添加: 申请费减免\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 申请费减免: 313/351\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 所属院系（英文）\n",
      "📄 目标文件: 所属院系英文_处理结果.csv\n",
      "🎯 添加字段: ['所属院系英文名称', '所属院系网址']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['所属院系英文名称', '所属院系网址']\n",
      "     📝 添加: 所属院系英文名称\n",
      "     📝 添加: 所属院系网址\n",
      "   ✅ 成功添加 2 个字段\n",
      "   📊 非空值统计: 所属院系英文名称: 347/351, 所属院系网址: 347/351\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 推荐信\n",
      "📄 目标文件: final_推荐信_gemini-2.5-flash_0_-1_processed.csv\n",
      "🎯 添加字段: ['推荐信']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['推荐信']\n",
      "     📝 添加: 推荐信\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 推荐信: 349/351\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 托福送分ETS code\n",
      "📄 目标文件: 托福送分ETS code_gemini-2.5-flash_0_-1_processed.csv\n",
      "🎯 添加字段: ['托福送分ETS code']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['托福送分ETS code']\n",
      "     📝 添加: 托福送分ETS code\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 托福送分ETS code: 350/351\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 项目标签\n",
      "📄 目标文件: 项目标签.csv\n",
      "🎯 添加字段: ['项目标签（英文）', '项目标签（中文）']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['项目标签（英文）', '项目标签（中文）']\n",
      "     📝 添加: 项目标签（英文）\n",
      "     📝 添加: 项目标签（中文）\n",
      "   ✅ 成功添加 2 个字段\n",
      "   📊 非空值统计: 项目标签（英文）: 350/351, 项目标签（中文）: 350/351\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 职业项目\n",
      "📄 目标文件: 职业项目.csv\n",
      "🎯 添加字段: ['职业项目']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['职业项目']\n",
      "     📝 添加: 职业项目\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 职业项目: 350/351\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: Capstone或Thesis\n",
      "📄 目标文件: Capstone或Thesis_修复版.csv\n",
      "🎯 添加字段: ['Gemini最终毕业要求']\n",
      "   📖 读取文件...\n",
      "   📊 找到 350 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 351, 字段文件: 350\n",
      "   📏 使用最小长度: 350\n",
      "   ➕ 添加字段: ['Gemini最终毕业要求']\n",
      "     📝 添加: Gemini最终毕业要求\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: Gemini最终毕业要求: 350/351\n",
      "----------------------------------------\n",
      "\n",
      "💾 保存整合结果...\n",
      "✅ 文件已保存至: /Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/DataScience/汇总/美研-数据科学.csv\n",
      "\n",
      "📊 整合完成统计:\n",
      "============================================================\n",
      "总字段组数: 10\n",
      "成功添加: 10\n",
      "添加失败: 0\n",
      "最终数据集大小: 351 行 × 34 列\n",
      "\n",
      "📋 详细处理结果:\n",
      "✅ 更多项目信息:\n",
      "     记录数: 350\n",
      "     添加的列: ['更多项目信息']\n",
      "✅ 课程网址:\n",
      "     记录数: 350\n",
      "     添加的列: ['课程网址']\n",
      "✅ 面试:\n",
      "     记录数: 350\n",
      "     添加的列: ['面试newp']\n",
      "✅ 申请费减免:\n",
      "     记录数: 350\n",
      "     添加的列: ['申请费减免']\n",
      "✅ 所属院系（英文）:\n",
      "     记录数: 350\n",
      "     添加的列: ['所属院系英文名称', '所属院系网址']\n",
      "✅ 推荐信:\n",
      "     记录数: 350\n",
      "     添加的列: ['推荐信']\n",
      "✅ 托福送分ETS code:\n",
      "     记录数: 350\n",
      "     添加的列: ['托福送分ETS code']\n",
      "✅ 项目标签:\n",
      "     记录数: 350\n",
      "     添加的列: ['项目标签（英文）', '项目标签（中文）']\n",
      "✅ 职业项目:\n",
      "     记录数: 350\n",
      "     添加的列: ['职业项目']\n",
      "✅ Capstone或Thesis:\n",
      "     记录数: 350\n",
      "     添加的列: ['Gemini最终毕业要求']\n",
      "\n",
      "🔍 最终数据质量检查:\n",
      "数据集形状: (351, 34)\n",
      "列名: ['大学排名', '大学名称', '大学英文名称', '所在城市', '专业中文名称', '学位', '专业英文名称', '所属院系', '专业领域', '课程长度', '申请费（美元)', '开学期', '截止日期', 'GPA', '托福', '雅思', 'GRE', 'GMAT', '学术背景', '材料要求', '招生网址', '专业网址', '更多项目信息', '课程网址', '面试newp', '申请费减免', '所属院系英文名称', '所属院系网址', '推荐信', '托福送分ETS code', '项目标签（英文）', '项目标签（中文）', '职业项目', 'Gemini最终毕业要求']\n",
      "\n",
      "📈 各字段覆盖率:\n",
      "   更多项目信息: 343/351 (97.7%)\n",
      "   课程网址: 160/351 (45.6%)\n",
      "   面试newp: 298/351 (84.9%)\n",
      "   申请费减免: 313/351 (89.2%)\n",
      "   所属院系英文名称: 347/351 (98.9%)\n",
      "   所属院系网址: 347/351 (98.9%)\n",
      "   推荐信: 349/351 (99.4%)\n",
      "   托福送分ETS code: 350/351 (99.7%)\n",
      "   项目标签（英文）: 350/351 (99.7%)\n",
      "   项目标签（中文）: 350/351 (99.7%)\n",
      "   职业项目: 350/351 (99.7%)\n",
      "   Gemini最终毕业要求: 350/351 (99.7%)\n",
      "\n",
      "🎉 整合完成! 最终数据集形状: (351, 34)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def integrate_fields_to_main_dataset():\n",
    "    \"\"\"\n",
    "    将各个字段文件夹中的CSV数据按行索引直接添加到主数据集中\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 整合后的数据集\n",
    "    \"\"\"\n",
    "    \n",
    "    # 主CSV文件路径\n",
    "    main_csv_path = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/DataScience/DataScience_df1.csv\"\n",
    "    \n",
    "    # 读取主数据集\n",
    "    print(\"📖 读取主数据集...\")\n",
    "    main_df = pd.read_csv(main_csv_path, encoding='utf-8-sig')\n",
    "    print(f\"主数据集包含 {len(main_df)} 个项目\")\n",
    "    \n",
    "    # 字段映射配置：文件夹名 -> (CSV文件名, 要提取的字段列表)\n",
    "    field_configs = {\n",
    "        '更多项目信息': ('更多项目信息_提取结果.csv', ['更多项目信息']),\n",
    "        '课程网址': ('课程网址_规则处理结果.csv', ['课程网址']),\n",
    "        '面试': ('面试需求_处理结果.csv', ['面试newp']),\n",
    "        # '其他字段': 跳过\n",
    "        '申请费减免': ('final_申请费减免_gemini-2.5-flash_median_accuracy_processed.csv', ['申请费减免']),\n",
    "        '所属院系（英文）': ('所属院系英文_处理结果.csv', ['所属院系英文名称', '所属院系网址']),\n",
    "        '推荐信': ('final_推荐信_gemini-2.5-flash_0_-1_processed.csv', ['推荐信']),\n",
    "        '托福送分ETS code': ('托福送分ETS code_gemini-2.5-flash_0_-1_processed.csv', ['托福送分ETS code']),\n",
    "        '项目标签': ('项目标签.csv', ['项目标签（英文）', '项目标签（中文）']),\n",
    "        '职业项目': ('职业项目.csv', ['职业项目']),\n",
    "        'Capstone或Thesis': ('Capstone或Thesis_修复版.csv', ['Gemini最终毕业要求'])\n",
    "    }\n",
    "    \n",
    "    # 基础路径\n",
    "    base_path = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/DataScience/fields_records\"\n",
    "    \n",
    "    # 记录处理统计\n",
    "    processing_stats = {\n",
    "        'total_fields': len(field_configs),\n",
    "        'successful_adds': 0,\n",
    "        'failed_adds': 0,\n",
    "        'field_details': {}\n",
    "    }\n",
    "    \n",
    "    print(\"\\n🔄 开始添加字段数据...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 逐个处理每个字段\n",
    "    for folder_name, (csv_filename, target_columns) in field_configs.items():\n",
    "        print(f\"\\n📁 处理文件夹: {folder_name}\")\n",
    "        print(f\"📄 目标文件: {csv_filename}\")\n",
    "        print(f\"🎯 添加字段: {target_columns}\")\n",
    "        \n",
    "        # 构建完整文件路径\n",
    "        csv_path = os.path.join(base_path, folder_name, csv_filename)\n",
    "        \n",
    "        field_stats = {\n",
    "            'csv_path': csv_path,\n",
    "            'target_columns': target_columns,\n",
    "            'file_exists': False,\n",
    "            'records_count': 0,\n",
    "            'columns_added': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 检查文件是否存在\n",
    "            if not os.path.exists(csv_path):\n",
    "                print(f\"   ❌ 文件不存在: {csv_path}\")\n",
    "                field_stats['error'] = \"文件不存在\"\n",
    "                processing_stats['failed_adds'] += 1\n",
    "                processing_stats['field_details'][folder_name] = field_stats\n",
    "                continue\n",
    "            \n",
    "            field_stats['file_exists'] = True\n",
    "            \n",
    "            # 读取字段CSV文件\n",
    "            print(f\"   📖 读取文件...\")\n",
    "            field_df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "            field_stats['records_count'] = len(field_df)\n",
    "            print(f\"   📊 找到 {len(field_df)} 条记录\")\n",
    "            \n",
    "            # 检查行数是否匹配\n",
    "            if len(field_df) != len(main_df):\n",
    "                print(f\"   ⚠️  警告: 行数不匹配! 主数据集: {len(main_df)}, 字段文件: {len(field_df)}\")\n",
    "                # 可以选择截断或填充，这里选择使用较短的长度\n",
    "                min_length = min(len(main_df), len(field_df))\n",
    "                print(f\"   📏 使用最小长度: {min_length}\")\n",
    "            else:\n",
    "                min_length = len(main_df)\n",
    "                print(f\"   ✅ 行数匹配: {min_length}\")\n",
    "            \n",
    "            # 检查所需列是否存在\n",
    "            available_columns = field_df.columns.tolist()\n",
    "            missing_columns = [col for col in target_columns if col not in available_columns]\n",
    "            \n",
    "            if missing_columns:\n",
    "                print(f\"   ⚠️  缺失字段: {missing_columns}\")\n",
    "                print(f\"   📋 可用字段: {available_columns}\")\n",
    "                # 只使用存在的字段\n",
    "                existing_columns = [col for col in target_columns if col in available_columns]\n",
    "                if not existing_columns:\n",
    "                    print(f\"   ❌ 没有可用的目标字段\")\n",
    "                    field_stats['error'] = \"没有可用的目标字段\"\n",
    "                    processing_stats['failed_adds'] += 1\n",
    "                    processing_stats['field_details'][folder_name] = field_stats\n",
    "                    continue\n",
    "                target_columns = existing_columns\n",
    "            \n",
    "            # 直接添加字段到主数据集\n",
    "            print(f\"   ➕ 添加字段: {target_columns}\")\n",
    "            added_columns = []\n",
    "            \n",
    "            for column in target_columns:\n",
    "                if column in field_df.columns:\n",
    "                    # 确保不会覆盖已存在的同名列\n",
    "                    new_column_name = column\n",
    "                    counter = 1\n",
    "                    while new_column_name in main_df.columns:\n",
    "                        new_column_name = f\"{column}_{counter}\"\n",
    "                        counter += 1\n",
    "                    \n",
    "                    # 添加列数据（截断到最小长度）\n",
    "                    main_df[new_column_name] = field_df[column].iloc[:min_length].reset_index(drop=True)\n",
    "                    added_columns.append(new_column_name)\n",
    "                    \n",
    "                    if new_column_name != column:\n",
    "                        print(f\"     📝 {column} → {new_column_name} (重命名避免冲突)\")\n",
    "                    else:\n",
    "                        print(f\"     📝 添加: {column}\")\n",
    "            \n",
    "            field_stats['columns_added'] = added_columns\n",
    "            \n",
    "            # 统计非空值\n",
    "            non_null_stats = []\n",
    "            for col in added_columns:\n",
    "                non_null_count = main_df[col].notna().sum()\n",
    "                non_null_stats.append(f\"{col}: {non_null_count}/{len(main_df)}\")\n",
    "            \n",
    "            print(f\"   ✅ 成功添加 {len(added_columns)} 个字段\")\n",
    "            print(f\"   📊 非空值统计: {', '.join(non_null_stats)}\")\n",
    "            \n",
    "            processing_stats['successful_adds'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 处理失败: {str(e)}\")\n",
    "            field_stats['error'] = str(e)\n",
    "            processing_stats['failed_adds'] += 1\n",
    "        \n",
    "        processing_stats['field_details'][folder_name] = field_stats\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # 保存结果\n",
    "    output_dir = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/DataScience/汇总\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, \"美研-数据科学.csv\")\n",
    "    \n",
    "    print(f\"\\n💾 保存整合结果...\")\n",
    "    main_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ 文件已保存至: {output_path}\")\n",
    "    \n",
    "    # 输出处理统计\n",
    "    print(f\"\\n📊 整合完成统计:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"总字段组数: {processing_stats['total_fields']}\")\n",
    "    print(f\"成功添加: {processing_stats['successful_adds']}\")\n",
    "    print(f\"添加失败: {processing_stats['failed_adds']}\")\n",
    "    print(f\"最终数据集大小: {main_df.shape[0]} 行 × {main_df.shape[1]} 列\")\n",
    "    \n",
    "    # 详细统计\n",
    "    print(f\"\\n📋 详细处理结果:\")\n",
    "    for folder_name, stats in processing_stats['field_details'].items():\n",
    "        status = \"✅\" if 'error' not in stats else \"❌\"\n",
    "        print(f\"{status} {folder_name}:\")\n",
    "        if 'error' in stats:\n",
    "            print(f\"     错误: {stats['error']}\")\n",
    "        else:\n",
    "            print(f\"     记录数: {stats['records_count']}\")\n",
    "            print(f\"     添加的列: {stats['columns_added']}\")\n",
    "    \n",
    "    # 检查最终数据质量\n",
    "    print(f\"\\n🔍 最终数据质量检查:\")\n",
    "    print(f\"数据集形状: {main_df.shape}\")\n",
    "    print(f\"列名: {list(main_df.columns)}\")\n",
    "    \n",
    "    # 显示每列的非空值统计\n",
    "    print(f\"\\n📈 各字段覆盖率:\")\n",
    "    total_records = len(main_df)\n",
    "    \n",
    "    # 只显示新添加的字段的统计\n",
    "    original_columns = ['大学排名', '大学名称', '大学英文名称', '所在城市', '专业中文名称', '学位', \n",
    "                       '专业英文名称', '所属院系', '专业领域', '课程长度', '申请费（美元)', '开学期', \n",
    "                       '截止日期', 'GPA', '托福', '雅思', 'GRE', 'GMAT', '学术背景', '材料要求', \n",
    "                       '招生网址', '专业网址']\n",
    "    \n",
    "    new_columns = [col for col in main_df.columns if col not in original_columns]\n",
    "    \n",
    "    for column in new_columns:\n",
    "        non_null_count = main_df[column].notna().sum()\n",
    "        coverage = (non_null_count / total_records) * 100\n",
    "        print(f\"   {column}: {non_null_count}/{total_records} ({coverage:.1f}%)\")\n",
    "    \n",
    "    return main_df, processing_stats\n",
    "\n",
    "# 运行整合函数\n",
    "if __name__ == \"__main__\":\n",
    "    final_df, stats = integrate_fields_to_main_dataset()\n",
    "    print(f\"\\n🎉 整合完成! 最终数据集形状: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
