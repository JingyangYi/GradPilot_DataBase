{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 10)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "sys.path.append('/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience')\n",
    "import asyncio\n",
    "from call_api import call_gemini, async_call_gemini\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "field_name = \"所属院系（英文）\"\n",
    "field_path = f\"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_csv/{field_name}.csv\"\n",
    "field_df = pd.read_csv(field_path)\n",
    "\n",
    "field_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an assistant whose only task is to identify the **school / college / faculty** that houses the graduate program below and return its official homepage URL.\n",
    "\n",
    "────────────────────────────────────────────────────────\n",
    "How to find the information  \n",
    "\n",
    "1. **Primary *.edu* sources only**  \n",
    "   • Inspect the Admissions and Program URLs provided.  \n",
    "   • Open additional pages within the same university’s *.edu* domain (e.g., “About the School”, “Departments”).  \n",
    "   ⛔  Ignore non-*.edu* sites, blogs, rankings, or third-party profiles.\n",
    "\n",
    "2. **Optional Google search**  \n",
    "   Single query:  \n",
    "   \"{university} {degree} {program} school college site:.edu\"  \n",
    "   Check only *.edu* results until you locate the school’s homepage.\n",
    "\n",
    "3. **Selection rules**  \n",
    "   • Capture *one* clear school/college name (e.g., “School of Engineering and Applied Science”).  \n",
    "   • Copy the most authoritative homepage URL for that school (https://…).  \n",
    "   • If no reliable *.edu* source identifies the school, output **Not found**.  \n",
    "   • Never invent.\n",
    "\n",
    "────────────────────────────────────────────────────────\n",
    "⚠️  Output format (exactly one line, no quotes, no extra text)  \n",
    "\n",
    "<School / College Name>, <URL>  \n",
    "or  \n",
    "Not found  \n",
    "\n",
    "Examples of valid outputs:  \n",
    "School of Engineering and Applied Science, https://engineering.columbia.edu  \n",
    "Graduate School of Arts and Sciences, https://gsas.harvard.edu  \n",
    "Not found  \n",
    "\n",
    "────────────────────────────────────────────────────────\n",
    "Pages to consult first:  \n",
    "• Admissions URL: {admissions_url}  \n",
    "• Program URL:    {program_url}\n",
    "\n",
    "What is the school/college and its homepage?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# Async Gemini wrapper\n",
    "from call_api import async_call_gemini\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Concurrency guard – avoid hitting rate-limits\n",
    "# ---------------------------------------------------------------------------\n",
    "semaphore = asyncio.Semaphore(4)            # max concurrent rows\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Per-row worker\n",
    "# ---------------------------------------------------------------------------\n",
    "async def process_row(row, prompt_template, num_vote: int, model_name: str):\n",
    "    \"\"\"\n",
    "    1. Format the prompt for this row\n",
    "    2. Launch `num_vote` Gemini calls in parallel\n",
    "    3. Capture BOTH normal answers *and* every possible error case\n",
    "    4. Return a serialisable record\n",
    "    \"\"\"\n",
    "    async with semaphore:\n",
    "        row    = row.to_dict()\n",
    "        prompt = prompt_template.format(\n",
    "            university     = row[\"大学英文名称\"],\n",
    "            degree         = row[\"学位\"],\n",
    "            program        = row[\"专业英文名称\"],\n",
    "            department     = row[\"所属院系\"],\n",
    "            admissions_url = row[\"招生网址\"],\n",
    "            program_url    = row[\"专业网址\"],\n",
    "        )\n",
    "\n",
    "        record: dict = row.copy()\n",
    "        record[\"llm_reponses\"] = {}\n",
    "\n",
    "        # -------- launch Gemini calls in parallel --------------------\n",
    "        tasks = [\n",
    "            async_call_gemini(\n",
    "                prompt,\n",
    "                model_name=model_name,\n",
    "                use_search=True,\n",
    "                url_context=True\n",
    "            )\n",
    "            for _ in range(num_vote)\n",
    "        ]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "        # -------- post-process each response -------------------------\n",
    "        for i, response in enumerate(responses):\n",
    "            resp_key = f\"response {i+1}\"\n",
    "\n",
    "            # -- 1. Transport / server-side errors (string starting \"Error:\")\n",
    "            if isinstance(response, str) and response.startswith(\"Error:\"):\n",
    "                record[\"llm_reponses\"][resp_key] = {\n",
    "                    \"error\": response                       # e.g. \"Error: 429 Rate limit …\"\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # -- 2. Empty / malformed response objects\n",
    "            if not hasattr(response, \"candidates\") or not response.candidates:\n",
    "                record[\"llm_reponses\"][resp_key] = {\n",
    "                    \"error\": \"No candidates returned\",\n",
    "                    \"raw_response\": str(response)\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # -- 3. Extract main answer text\n",
    "            try:\n",
    "                text = response.candidates[0].content.parts[0].text\n",
    "            except Exception as e:\n",
    "                record[\"llm_reponses\"][resp_key] = {\n",
    "                    \"error\": f\"Cannot parse text: {e}\",\n",
    "                    \"raw_response\": str(response)\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # -- 4. Extract additional metadata (best-effort)\n",
    "            try:\n",
    "                url_context = str(response.candidates[0].url_context_metadata)\n",
    "            except Exception:\n",
    "                url_context = \"Not used\"\n",
    "\n",
    "            try:\n",
    "                search_pages = (\n",
    "                    f\"Search Chunks: \"\n",
    "                    f\"{response.candidates[0].grounding_metadata.grounding_chunks}\"\n",
    "                )\n",
    "            except Exception:\n",
    "                search_pages = \"Not used\"\n",
    "\n",
    "            try:\n",
    "                search_queries = (\n",
    "                    f\"Search Query: \"\n",
    "                    f\"{response.candidates[0].grounding_metadata.web_search_queries}\"\n",
    "                )\n",
    "            except Exception:\n",
    "                search_queries = \"Not used\"\n",
    "\n",
    "            try:\n",
    "                search_support = (\n",
    "                    f\"Search Supports: \"\n",
    "                    f\"{response.candidates[0].grounding_metadata.groundingSupports}\"\n",
    "                )\n",
    "            except Exception:\n",
    "                search_support = \"Not used\"\n",
    "\n",
    "            # -- 5. Store normal answer + metadata + raw object\n",
    "            record[\"llm_reponses\"][resp_key] = {\n",
    "                \"response_text\": text,\n",
    "                \"url_context\": url_context,\n",
    "                \"search_queries\": search_queries,\n",
    "                \"search_pages\": search_pages,\n",
    "                \"search_support\": search_support,\n",
    "                \"raw_response\": str(response)             # keep for deep-debugging\n",
    "            }\n",
    "\n",
    "        return record\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Batch orchestrator with tqdm progress bar\n",
    "# ---------------------------------------------------------------------------\n",
    "async def request_and_store_async(prompt_template,\n",
    "                                  field_df,\n",
    "                                  num_vote: int,\n",
    "                                  model_name: str,\n",
    "                                  start_from: int = 0,\n",
    "                                  end_at: int = -1):\n",
    "    \"\"\"\n",
    "    Runs `process_row` over the dataframe slice asynchronously,\n",
    "    shows a live tqdm bar, and dumps the results to JSON.\n",
    "    \"\"\"\n",
    "    df = field_df.copy()[start_from:end_at]\n",
    "\n",
    "    # Spawn tasks for every row in the slice\n",
    "    tasks = [\n",
    "        process_row(row, prompt_template, num_vote, model_name)\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # tqdm_asyncio.gather gives us progress updates as tasks complete\n",
    "    response_records = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    # Persist to disk ------------------------------------------------\n",
    "    output_dir = f\"../fields_records/{field_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{field_name}_{model_name}_{start_from}_{end_at}.json\"\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(response_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return response_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m end_at \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(field_df)\n\u001b[1;32m      7\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.5-flash\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m response_records \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m      9\u001b[0m     request_and_store_async(prompt_template, field_df, num_vote, model_name, start_from\u001b[38;5;241m=\u001b[39mstart_from, end_at\u001b[38;5;241m=\u001b[39mend_at)\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/selectors.py:566\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mcontrol(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Only needed in Jupyter\n",
    "\n",
    "num_vote = 3\n",
    "start_from = 0\n",
    "end_at = len(field_df)\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "response_records = asyncio.run(\n",
    "    request_and_store_async(prompt_template, field_df, num_vote, model_name, start_from=start_from, end_at=end_at)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意调整文件路径！！！！！！！！！\n",
    "\n",
    "json_file = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/所属院系（英文）/所属院系（英文）_gemini-2.5-flash_0_-1.json\"\n",
    "output_csv = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/所属院系（英文）/所属院系英文_处理结果.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_department_from_json(json_file_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    从JSON文件中提取所属院系（英文）信息\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: JSON文件路径\n",
    "        output_csv_path: 输出CSV文件路径\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: 处理后的数据\n",
    "    \"\"\"\n",
    "    \n",
    "    # 读取JSON文件\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for record in data:\n",
    "        # 提取基本信息\n",
    "        basic_info = {\n",
    "            '大学英文名称': record.get('大学英文名称', ''),\n",
    "            '学位': record.get('学位', ''),\n",
    "            '专业英文名称': record.get('专业英文名称', ''),\n",
    "            '所属院系': record.get('所属院系', ''),\n",
    "            '招生网址': record.get('招生网址', ''),\n",
    "            '专业网址': record.get('专业网址', ''),\n",
    "        }\n",
    "        \n",
    "        # 提取LLM响应\n",
    "        llm_responses = record.get('llm_reponses', {})\n",
    "        \n",
    "        # 提取三次回答的response_text\n",
    "        response_texts = []\n",
    "        for i in range(1, 4):\n",
    "            response_key = f\"response {i}\"\n",
    "            if response_key in llm_responses:\n",
    "                response_text = llm_responses[response_key].get('response_text', '')\n",
    "                response_texts.append(response_text)\n",
    "            else:\n",
    "                response_texts.append('')\n",
    "        \n",
    "        # 比较三个回答的一致性并选择最终答案\n",
    "        selected_response, consistency_info, valid_responses = select_best_response(response_texts)\n",
    "        \n",
    "        # 解析选中的回答，提取院系名称和URL\n",
    "        department_name, department_url = parse_response_text(selected_response)\n",
    "        \n",
    "        # 构建最终记录\n",
    "        final_record = basic_info.copy()\n",
    "        final_record.update({\n",
    "            '所属院系（英文）': department_name,\n",
    "            '所属院系网址': department_url,\n",
    "            'response_1': response_texts[0] if len(response_texts) > 0 else '',\n",
    "            'response_2': response_texts[1] if len(response_texts) > 1 else '',\n",
    "            'response_3': response_texts[2] if len(response_texts) > 2 else '',\n",
    "            'response_1_valid': is_valid_response(response_texts[0]) if len(response_texts) > 0 else False,\n",
    "            'response_2_valid': is_valid_response(response_texts[1]) if len(response_texts) > 1 else False,\n",
    "            'response_3_valid': is_valid_response(response_texts[2]) if len(response_texts) > 2 else False,\n",
    "            'valid_response_count': len(valid_responses),\n",
    "            'consistency_type': consistency_info['type'],\n",
    "            'consistency_detail': consistency_info['detail']\n",
    "        })\n",
    "        \n",
    "        results.append(final_record)\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # 重新排列列的顺序，确保重要信息在前面\n",
    "    column_order = [\n",
    "        '大学英文名称', '学位', '专业英文名称', '所属院系', \n",
    "        '所属院系（英文）', '所属院系网址',\n",
    "        '招生网址', '专业网址',\n",
    "        'valid_response_count', 'consistency_type', 'consistency_detail',\n",
    "        'response_1', 'response_1_valid',\n",
    "        'response_2', 'response_2_valid', \n",
    "        'response_3', 'response_3_valid'\n",
    "    ]\n",
    "    \n",
    "    # 确保所有列都存在\n",
    "    for col in column_order:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "    \n",
    "    # 按指定顺序重排列\n",
    "    df = df[column_order]\n",
    "    \n",
    "    # 保存为CSV\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 打印统计信息\n",
    "    print(f\"数据处理完成！\")\n",
    "    print(f\"总共处理了 {len(df)} 条记录\")\n",
    "    print(f\"有效回答统计:\")\n",
    "    valid_counts = df['valid_response_count'].value_counts().sort_index()\n",
    "    for count, records in valid_counts.items():\n",
    "        print(f\"  {count}个有效回答: {records} 条记录\")\n",
    "    print(f\"一致性统计:\")\n",
    "    consistency_counts = df['consistency_type'].value_counts()\n",
    "    for consistency_type, count in consistency_counts.items():\n",
    "        print(f\"  {consistency_type}: {count} 条\")\n",
    "    \n",
    "    # 统计成功提取的院系信息\n",
    "    successful_extractions = len(df[df['所属院系（英文）'] != ''])\n",
    "    print(f\"成功提取院系信息: {successful_extractions}/{len(df)} ({successful_extractions/len(df):.1%})\")\n",
    "    \n",
    "    print(f\"数据已保存到: {output_csv_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def is_valid_response(response_text):\n",
    "    \"\"\"\n",
    "    检查回答是否符合\"school, url\"格式\n",
    "    \n",
    "    Args:\n",
    "        response_text: LLM的回答文本\n",
    "    \n",
    "    Returns:\n",
    "        bool: 是否为合法格式\n",
    "    \"\"\"\n",
    "    \n",
    "    if not response_text or not response_text.strip():\n",
    "        return False\n",
    "    \n",
    "    # 去除首尾空白\n",
    "    text = response_text.strip()\n",
    "    \n",
    "    # 检查是否包含逗号\n",
    "    if ',' not in text:\n",
    "        return False\n",
    "    \n",
    "    # 检查长度（过长的回答通常是解释性文本）\n",
    "    if len(text) > 200:  # 设置最大长度限制\n",
    "        return False\n",
    "    \n",
    "    # 检查是否包含URL\n",
    "    url_pattern = r'https?://[^\\s,]+'\n",
    "    if not re.search(url_pattern, text):\n",
    "        return False\n",
    "    \n",
    "    # 检查是否包含某些无效关键词\n",
    "    invalid_keywords = [\n",
    "        'not found', 'not available', 'unable to find', 'could not find',\n",
    "        'no information', 'not mentioned', 'unclear', 'not specified',\n",
    "        'the browse results', 'clearly indicate', 'according to'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    for keyword in invalid_keywords:\n",
    "        if keyword in text_lower:\n",
    "            return False\n",
    "    \n",
    "    # 分割并检查格式\n",
    "    parts = text.split(',')\n",
    "    if len(parts) < 2:\n",
    "        return False\n",
    "    \n",
    "    # 第一部分应该是学院名称（不应该太短或包含特殊词汇）\n",
    "    school_part = parts[0].strip()\n",
    "    if len(school_part) < 5:  # 学院名称不应该太短\n",
    "        return False\n",
    "    \n",
    "    # 第二部分应该包含URL\n",
    "    url_part = ','.join(parts[1:]).strip()\n",
    "    if not re.search(url_pattern, url_part):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def select_best_response(response_texts):\n",
    "    \"\"\"\n",
    "    根据一致性选择最佳回答（只考虑合法格式的回答）\n",
    "    \n",
    "    Args:\n",
    "        response_texts: 三个LLM回答的列表\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (选中的回答, 一致性信息, 有效回答列表)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 过滤出符合格式的有效回答\n",
    "    valid_responses = [resp for resp in response_texts if is_valid_response(resp)]\n",
    "    \n",
    "    if not valid_responses:\n",
    "        return '', {'type': 'no_valid_response', 'detail': 'No valid responses found'}, []\n",
    "    \n",
    "    if len(valid_responses) == 1:\n",
    "        return valid_responses[0], {'type': 'single_valid_response', 'detail': 'Only one valid response'}, valid_responses\n",
    "    \n",
    "    # 统计每个有效回答出现的次数\n",
    "    response_counts = Counter(valid_responses)\n",
    "    most_common = response_counts.most_common()\n",
    "    \n",
    "    # 完全一致（所有有效回答都相同）\n",
    "    if len(most_common) == 1:\n",
    "        return most_common[0][0], {'type': 'all_valid_same', 'detail': f'All {most_common[0][1]} valid responses are identical'}, valid_responses\n",
    "    \n",
    "    # 两个或更多相同，选择出现次数最多的\n",
    "    if most_common[0][1] >= 2:\n",
    "        return most_common[0][0], {'type': 'majority_valid_same', 'detail': f'{most_common[0][1]} valid responses are identical'}, valid_responses\n",
    "    \n",
    "    # 所有有效回答都不同，随机选择一个\n",
    "    selected = random.choice(valid_responses)\n",
    "    return selected, {'type': 'all_valid_different', 'detail': 'All valid responses are different, randomly selected'}, valid_responses\n",
    "\n",
    "def parse_response_text(response_text):\n",
    "    \"\"\"\n",
    "    解析回答文本，提取院系名称和URL\n",
    "    \n",
    "    Args:\n",
    "        response_text: LLM的回答文本\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (院系名称, URL)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not response_text.strip():\n",
    "        return '', ''\n",
    "    \n",
    "    # 按逗号分割\n",
    "    parts = response_text.split(',')\n",
    "    \n",
    "    if len(parts) >= 2:\n",
    "        department_name = parts[0].strip()\n",
    "        \n",
    "        # 在第二部分及之后寻找URL\n",
    "        url_part = ','.join(parts[1:]).strip()\n",
    "        \n",
    "        # 提取URL\n",
    "        url_pattern = r'https?://[^\\s,]+'\n",
    "        url_match = re.search(url_pattern, url_part)\n",
    "        \n",
    "        if url_match:\n",
    "            department_url = url_match.group()\n",
    "            # 清理URL末尾的标点符号\n",
    "            department_url = re.sub(r'[,\\.\\s]+$', '', department_url)\n",
    "        else:\n",
    "            department_url = ''\n",
    "    \n",
    "    else:\n",
    "        # 如果没有逗号，整个文本作为院系名称\n",
    "        department_name = response_text.strip()\n",
    "        department_url = ''\n",
    "    \n",
    "    return department_name, department_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成！\n",
      "总共处理了 120 条记录\n",
      "有效回答统计:\n",
      "  0个有效回答: 2 条记录\n",
      "  1个有效回答: 4 条记录\n",
      "  2个有效回答: 24 条记录\n",
      "  3个有效回答: 90 条记录\n",
      "一致性统计:\n",
      "  all_valid_same: 51 条\n",
      "  majority_valid_same: 41 条\n",
      "  all_valid_different: 22 条\n",
      "  single_valid_response: 4 条\n",
      "  no_valid_response: 2 条\n",
      "成功提取院系信息: 118/120 (98.3%)\n",
      "数据已保存到: /Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/所属院系（英文）/所属院系英文_处理结果.csv\n",
      "\n",
      "前5条记录预览：\n",
      "                                  大学英文名称  \\\n",
      "0  Massachusetts Institute of Technology   \n",
      "1  Massachusetts Institute of Technology   \n",
      "2  Massachusetts Institute of Technology   \n",
      "3                     Harvard University   \n",
      "4                     Harvard University   \n",
      "\n",
      "                                              专业英文名称  \\\n",
      "0  Civil and Environmental Engineering：Data Scien...   \n",
      "1                     Social and Engineering Systems   \n",
      "2            Data, Economics, and Development Policy   \n",
      "3                            Master in Public Policy   \n",
      "4  Master in Public Administration in Internation...   \n",
      "\n",
      "                                            所属院系（英文）  \\\n",
      "0  Department of Civil and Environmental Engineering   \n",
      "1                                 Institute for Data   \n",
      "2                            Department of Economics   \n",
      "3                             Harvard Kennedy School   \n",
      "4                             Harvard Kennedy School   \n",
      "\n",
      "                        所属院系网址  valid_response_count     consistency_type  \n",
      "0          https://cee.mit.edu                     3       all_valid_same  \n",
      "1         https://idss.mit.edu                     3  all_valid_different  \n",
      "2    https://economics.mit.edu                     3  majority_valid_same  \n",
      "3  https://www.hks.harvard.edu                     3       all_valid_same  \n",
      "4  https://www.hks.harvard.edu                     3       all_valid_same  \n",
      "\n",
      "=== 成功提取院系信息的示例 ===\n",
      "大学: Massachusetts Institute of Technology\n",
      "专业: Civil and Environmental Engineering：Data Science for Engineering Systems (DSES) track\n",
      "原始院系: 土木与环境工程系\n",
      "英文院系: Department of Civil and Environmental Engineering\n",
      "院系网址: https://cee.mit.edu\n",
      "有效回答数: 3\n",
      "一致性: all_valid_same\n",
      "--------------------------------------------------\n",
      "大学: Massachusetts Institute of Technology\n",
      "专业: Social and Engineering Systems\n",
      "原始院系:  \n",
      "数据，系统和社会研究所\n",
      "英文院系: Institute for Data\n",
      "院系网址: https://idss.mit.edu\n",
      "有效回答数: 3\n",
      "一致性: all_valid_different\n",
      "--------------------------------------------------\n",
      "大学: Massachusetts Institute of Technology\n",
      "专业: Data, Economics, and Development Policy\n",
      "原始院系: 经济学系\n",
      "英文院系: Department of Economics\n",
      "院系网址: https://economics.mit.edu\n",
      "有效回答数: 3\n",
      "一致性: majority_valid_same\n",
      "--------------------------------------------------\n",
      "大学: Harvard University\n",
      "专业: Master in Public Policy\n",
      "原始院系: 哈佛大学肯尼迪学院\n",
      "英文院系: Harvard Kennedy School\n",
      "院系网址: https://www.hks.harvard.edu\n",
      "有效回答数: 3\n",
      "一致性: all_valid_same\n",
      "--------------------------------------------------\n",
      "大学: Harvard University\n",
      "专业: Master in Public Administration in International Development\n",
      "原始院系: 哈佛大学肯尼迪学院\n",
      "英文院系: Harvard Kennedy School\n",
      "院系网址: https://www.hks.harvard.edu\n",
      "有效回答数: 3\n",
      "一致性: all_valid_same\n",
      "--------------------------------------------------\n",
      "\n",
      "=== 未能提取院系信息的示例 ===\n",
      "大学: University of Washington\n",
      "专业: Atmospheric Sciences (PhD)： DATA SCIENCE  TRACK\n",
      "原始院系: 环境学院\n",
      "回答1:  (有效: False)\n",
      "回答2:  (有效: False)\n",
      "回答3: Not found (有效: False)\n",
      "原因: No valid responses found\n",
      "--------------------------------------------------\n",
      "大学: University of Miami\n",
      "专业:  Master of Science in Data Science：Marine & Atmospheric Science (Rosenstiel School of Marine & Atmospheric Sciences)\n",
      "原始院系: 文理学院\n",
      "回答1: The browsed pages provide a clear answer. The \"Academic Tracks\" page states under \"Marine and Atmospheric Science\" track that \"Most courses in this concentration are taken within the Rosenstiel School of Marine, Atmospheric, and Earth Sciences.\"\n",
      "\n",
      "The \"International Students\" page also lists \"Rosenstiel School of Marine, Atmospheric, and Earth Science\" under the \"Schools\" menu, which leads to the URL https://welcome.rsmas.miami.edu/. This appears to be the official homepage.\n",
      "\n",
      "Therefore, the school is the Rosenstiel School of Marine, Atmospheric, and Earth Sciences, and its homepage is https://welcome.rsmas.miami.edu/.\n",
      "\n",
      "Rosenstiel School of Marine, Atmospheric, and Earth Sciences, https://welcome.rsmas.miami.edu/\n",
      " (有效: False)\n",
      "回答2:  (有效: False)\n",
      "回答3: The browsed pages confirm that the \"Rosenstiel School of Marine, Atmospheric, and Earth Science\" is the relevant school. Both pages list \"Rosenstiel School of Marine, Atmospheric, and Earth Science\" under \"Schools\" in the header/navigation.\n",
      "\n",
      "Now I need to find the homepage URL for the Rosenstiel School. The current URLs are for the Master of Science in Data Science program, which is housed within that school. I'll search for \"Rosenstiel School of Marine, Atmospheric, and Earth Science\" on the University of Miami site, or directly search for its homepage.\n",
      "\n",
      "Since the program explicitly mentions \"Rosenstiel School of Marine & Atmospheric Sciences\", I will prioritize finding the homepage for that school. The full name appears to be \"Rosenstiel School of Marine, Atmospheric, and Earth Science\" based on the browse results.\n",
      "\n",
      "Let's try a concise search for \"Rosenstiel School of Marine, Atmospheric, and Earth Science site:.edu\" to find its homepage. (有效: False)\n",
      "原因: No valid responses found\n",
      "--------------------------------------------------\n",
      "\n",
      "=== 数据完整性检查 ===\n",
      "有院系名称的记录: 118/120\n",
      "有院系网址的记录: 118/120\n",
      "同时有名称和网址的记录: 118/120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = extract_department_from_json(json_file, output_csv)\n",
    "\n",
    "# 显示前几条记录\n",
    "print(\"\\n前5条记录预览：\")\n",
    "display_columns = ['大学英文名称', '专业英文名称', '所属院系（英文）', '所属院系网址', 'valid_response_count', 'consistency_type']\n",
    "print(df[display_columns].head())\n",
    "\n",
    "# 显示成功提取的示例\n",
    "print(f\"\\n=== 成功提取院系信息的示例 ===\")\n",
    "successful = df[df['所属院系（英文）'] != ''].head(5)\n",
    "for _, row in successful.iterrows():\n",
    "    print(f\"大学: {row['大学英文名称']}\")\n",
    "    print(f\"专业: {row['专业英文名称']}\")\n",
    "    print(f\"原始院系: {row['所属院系']}\")\n",
    "    print(f\"英文院系: {row['所属院系（英文）']}\")\n",
    "    print(f\"院系网址: {row['所属院系网址']}\")\n",
    "    print(f\"有效回答数: {row['valid_response_count']}\")\n",
    "    print(f\"一致性: {row['consistency_type']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 显示失败的示例\n",
    "print(f\"\\n=== 未能提取院系信息的示例 ===\")\n",
    "failed = df[df['所属院系（英文）'] == ''].head(3)\n",
    "for _, row in failed.iterrows():\n",
    "    print(f\"大学: {row['大学英文名称']}\")\n",
    "    print(f\"专业: {row['专业英文名称']}\")\n",
    "    print(f\"原始院系: {row['所属院系']}\")\n",
    "    print(f\"回答1: {row['response_1']} (有效: {row['response_1_valid']})\")\n",
    "    print(f\"回答2: {row['response_2']} (有效: {row['response_2_valid']})\")\n",
    "    print(f\"回答3: {row['response_3']} (有效: {row['response_3_valid']})\")\n",
    "    print(f\"原因: {row['consistency_detail']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 验证数据完整性\n",
    "print(f\"\\n=== 数据完整性检查 ===\")\n",
    "has_department_name = len(df[df['所属院系（英文）'] != ''])\n",
    "has_department_url = len(df[df['所属院系网址'] != ''])\n",
    "has_both = len(df[(df['所属院系（英文）'] != '') & (df['所属院系网址'] != '')])\n",
    "\n",
    "print(f\"有院系名称的记录: {has_department_name}/{len(df)}\")\n",
    "print(f\"有院系网址的记录: {has_department_url}/{len(df)}\")\n",
    "print(f\"同时有名称和网址的记录: {has_both}/{len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
