{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 9)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "sys.path.append('/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience')\n",
    "import asyncio\n",
    "from call_api import call_gemini, async_call_gemini\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "field_name = \"项目标签\"\n",
    "field_path = f\"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_csv/{field_name}.csv\"\n",
    "field_df = pd.read_csv(field_path)\n",
    "\n",
    "field_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Your task is to summarize the program characteristics of a graduate program from a university based on the content of the provided URL.\n",
    "First, please carefully access and read the content of the following URL:\n",
    "<URL>\n",
    "{admissions_url}\n",
    "{program_url}\n",
    "</URL>\n",
    "When summarizing the program characteristics, focus on the following aspects:\n",
    "1. Disciplinary characteristics: Identify the main disciplines involved in the program, such as math, statistics, data science.\n",
    "2. Interdisciplinary nature: Determine whether the program is an interdisciplinary program.\n",
    "When extracting information from the URL content, be thorough and precise. One program can have multiple characteristics. For each program, you should return no more than 5 characteristics. \n",
    "If the program urls provides more than 5 keywords, you should return the most important 5 keywords.\n",
    "\n",
    "Please output the identified characteristics as tags. For example, if the program involves math and statistics, is interdisciplinary, the output could be:\n",
    "\n",
    "<Tag>math</Tag>\n",
    "<Tag>statistics</Tag>\n",
    "<Tag>interdisciplinary</Tag>\n",
    "\n",
    "Start summarizing the program characteristics now. Remember to output the characteristics in the <Tag> tags and return nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "\n",
    "# Async Gemini wrapper\n",
    "from call_api import async_call_gemini\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Concurrency guard – avoid hitting rate-limits\n",
    "# ---------------------------------------------------------------------------\n",
    "semaphore = asyncio.Semaphore(5)            # max concurrent rows\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Per-row worker\n",
    "# ---------------------------------------------------------------------------\n",
    "async def process_row(row, prompt_template, num_vote: int, model_name: str):\n",
    "    \"\"\"\n",
    "    1. Format the prompt for this row\n",
    "    2. Launch `num_vote` Gemini calls in parallel\n",
    "    3. Capture BOTH normal answers *and* every possible error case\n",
    "    4. Return a serialisable record\n",
    "    \"\"\"\n",
    "    async with semaphore:\n",
    "        row    = row.to_dict()\n",
    "        prompt = prompt_template.format(\n",
    "            university     = row[\"大学英文名称\"],\n",
    "            degree         = row[\"学位\"],\n",
    "            program        = row[\"专业英文名称\"],\n",
    "            department     = row[\"所属院系（英文）\"],\n",
    "            admissions_url = row[\"招生网址\"],\n",
    "            program_url    = row[\"专业网址\"],\n",
    "        )\n",
    "\n",
    "        record: dict = row.copy()\n",
    "        record[\"llm_reponses\"] = {}\n",
    "\n",
    "        # -------- launch Gemini calls in parallel --------------------\n",
    "        tasks = [\n",
    "            async_call_gemini(\n",
    "                prompt,\n",
    "                model_name=model_name,\n",
    "                use_search=True,\n",
    "                url_context=True\n",
    "            )\n",
    "            for _ in range(num_vote)\n",
    "        ]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "        # -------- post-process each response -------------------------\n",
    "        for i, response in enumerate(responses):\n",
    "            resp_key = f\"response {i+1}\"\n",
    "\n",
    "            # -- 1. Transport / server-side errors (string starting \"Error:\")\n",
    "            if isinstance(response, str) and response.startswith(\"Error:\"):\n",
    "                record[\"llm_reponses\"][resp_key] = {\n",
    "                    \"error\": response                       # e.g. \"Error: 429 Rate limit …\"\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # -- 2. Empty / malformed response objects\n",
    "            if not hasattr(response, \"candidates\") or not response.candidates:\n",
    "                record[\"llm_reponses\"][resp_key] = {\n",
    "                    \"error\": \"No candidates returned\",\n",
    "                    \"raw_response\": str(response)\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # -- 3. Extract main answer text\n",
    "            try:\n",
    "                text = response.candidates[0].content.parts[0].text\n",
    "            except Exception as e:\n",
    "                record[\"llm_reponses\"][resp_key] = {\n",
    "                    \"error\": f\"Cannot parse text: {e}\",\n",
    "                    \"raw_response\": str(response)\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # -- 4. Extract additional metadata (best-effort)\n",
    "            try:\n",
    "                url_context = str(response.candidates[0].url_context_metadata)\n",
    "            except Exception:\n",
    "                url_context = \"Not used\"\n",
    "\n",
    "            try:\n",
    "                search_pages = (\n",
    "                    f\"Search Chunks: \"\n",
    "                    f\"{response.candidates[0].grounding_metadata.grounding_chunks}\"\n",
    "                )\n",
    "            except Exception:\n",
    "                search_pages = \"Not used\"\n",
    "\n",
    "            try:\n",
    "                search_queries = (\n",
    "                    f\"Search Query: \"\n",
    "                    f\"{response.candidates[0].grounding_metadata.web_search_queries}\"\n",
    "                )\n",
    "            except Exception:\n",
    "                search_queries = \"Not used\"\n",
    "\n",
    "            try:\n",
    "                search_support = (\n",
    "                    f\"Search Supports: \"\n",
    "                    f\"{response.candidates[0].grounding_metadata.groundingSupports}\"\n",
    "                )\n",
    "            except Exception:\n",
    "                search_support = \"Not used\"\n",
    "\n",
    "            # -- 5. Store normal answer + metadata + raw object\n",
    "            record[\"llm_reponses\"][resp_key] = {\n",
    "                \"response_text\": text,\n",
    "                \"url_context\": url_context,\n",
    "                \"search_queries\": search_queries,\n",
    "                \"search_pages\": search_pages,\n",
    "                \"search_support\": search_support,\n",
    "                \"raw_response\": str(response)             # keep for deep-debugging\n",
    "            }\n",
    "\n",
    "        return record\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Batch orchestrator with tqdm progress bar\n",
    "# ---------------------------------------------------------------------------\n",
    "async def request_and_store_async(prompt_template,\n",
    "                                  field_df,\n",
    "                                  num_vote: int,\n",
    "                                  model_name: str,\n",
    "                                  start_from: int = 0,\n",
    "                                  end_at: int = -1):\n",
    "    \"\"\"\n",
    "    Runs `process_row` over the dataframe slice asynchronously,\n",
    "    shows a live tqdm bar, and dumps the results to JSON.\n",
    "    \"\"\"\n",
    "    df = field_df.copy()[start_from:end_at]\n",
    "\n",
    "    # Spawn tasks for every row in the slice\n",
    "    tasks = [\n",
    "        process_row(row, prompt_template, num_vote, model_name)\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # tqdm_asyncio.gather gives us progress updates as tasks complete\n",
    "    response_records = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "    # Persist to disk ------------------------------------------------\n",
    "    output_dir = f\"../fields_records/{field_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = f\"{output_dir}/{field_name}_{model_name}_{start_from}_{end_at}.json\"\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(response_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return response_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [07:10<00:00,  3.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Only needed in Jupyter\n",
    "\n",
    "num_vote = 3\n",
    "start_from = 0\n",
    "end_at = len(field_df)\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "response_records = asyncio.run(\n",
    "    request_and_store_async(prompt_template, field_df, num_vote, model_name, start_from=start_from, end_at=end_at)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = '/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/项目标签/项目标签_gemini-2.5-flash_0_121.json'\n",
    "output_csv_path = '/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/项目标签/项目标签_gemini-2.5-flash.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "项目标签提取统计:\n",
      "总项目数: 121\n",
      "第一次回答有效: 107 (88.4%)\n",
      "使用第二次回答: 8 (6.6%)\n",
      "使用第三次回答: 1 (0.8%)\n",
      "需要额外确认: 5 (4.1%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_project_tags(json_file_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Extract project tags from JSON file with fallback logic for invalid responses.\n",
    "    \n",
    "    Args:\n",
    "        json_file_path (str): Path to the JSON file containing project tag data\n",
    "        output_csv_path (str): Path to save the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with extracted results and statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    def is_valid_response(response_text):\n",
    "        \"\"\"Check if a response is valid (contains proper <Tag> format)\"\"\"\n",
    "        if not response_text or response_text.strip() == \"\":\n",
    "            return False\n",
    "        if \"not found\" in response_text.lower():\n",
    "            return False\n",
    "        if len(response_text) > 1000:  # Too long response\n",
    "            return False\n",
    "        \n",
    "        # Check if contains proper <Tag> format\n",
    "        tag_pattern = r'<[Tt]ag>([^<]+)</[Tt]ag>'\n",
    "        tags = re.findall(tag_pattern, response_text)\n",
    "        return len(tags) > 0\n",
    "    \n",
    "    def extract_tags_from_response(response_text):\n",
    "        \"\"\"Extract tags from a valid response\"\"\"\n",
    "        tag_pattern = r'<[Tt]ag>([^<]+)</[Tt]ag>'\n",
    "        tags = re.findall(tag_pattern, response_text)\n",
    "        # Clean and format tags\n",
    "        cleaned_tags = [tag.strip().title() for tag in tags if tag.strip()]\n",
    "        return \", \".join(cleaned_tags)\n",
    "    \n",
    "    # Load JSON data\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    results = []\n",
    "    stats = {\n",
    "        'total_programs': 0,\n",
    "        'first_response_valid': 0,\n",
    "        'second_response_used': 0,\n",
    "        'third_response_used': 0,\n",
    "        'need_confirmation': 0\n",
    "    }\n",
    "    \n",
    "    for program in data:\n",
    "        stats['total_programs'] += 1\n",
    "        \n",
    "        # Extract basic info\n",
    "        university = program.get('大学英文名称', '')\n",
    "        degree = program.get('学位', '')\n",
    "        major = program.get('专业英文名称', '')\n",
    "        school = program.get('所属院系', '')\n",
    "        \n",
    "        # Extract responses\n",
    "        llm_responses = program.get('llm_reponses', {})\n",
    "        \n",
    "        # Try responses in order: response 1, response 2, response 3\n",
    "        final_tags = \"需要额外确认\"\n",
    "        response_used = \"none\"\n",
    "        \n",
    "        for i in range(1, 4):\n",
    "            response_key = f\"response {i}\"\n",
    "            if response_key in llm_responses:\n",
    "                response_text = llm_responses[response_key].get('response_text', '')\n",
    "                \n",
    "                if is_valid_response(response_text):\n",
    "                    final_tags = extract_tags_from_response(response_text)\n",
    "                    response_used = f\"response_{i}\"\n",
    "                    \n",
    "                    if i == 1:\n",
    "                        stats['first_response_valid'] += 1\n",
    "                    elif i == 2:\n",
    "                        stats['second_response_used'] += 1\n",
    "                    elif i == 3:\n",
    "                        stats['third_response_used'] += 1\n",
    "                    break\n",
    "        \n",
    "        if final_tags == \"需要额外确认\":\n",
    "            stats['need_confirmation'] += 1\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            '大学英文名称': university,\n",
    "            '学位': degree,\n",
    "            '专业英文名称': major,\n",
    "            '所属院系': school,\n",
    "            '项目标签': final_tags,\n",
    "            'response_used': response_used\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"项目标签提取统计:\")\n",
    "    print(f\"总项目数: {stats['total_programs']}\")\n",
    "    print(f\"第一次回答有效: {stats['first_response_valid']} ({stats['first_response_valid']/stats['total_programs']*100:.1f}%)\")\n",
    "    print(f\"使用第二次回答: {stats['second_response_used']} ({stats['second_response_used']/stats['total_programs']*100:.1f}%)\")\n",
    "    print(f\"使用第三次回答: {stats['third_response_used']} ({stats['third_response_used']/stats['total_programs']*100:.1f}%)\")\n",
    "    print(f\"需要额外确认: {stats['need_confirmation']} ({stats['need_confirmation']/stats['total_programs']*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = extract_project_tags(\n",
    "    json_file_path,\n",
    "    output_csv_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/项目标签/项目标签_gemini-2.5-flash.csv\"\n",
    "json_file = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_notebook/tags.json\"\n",
    "output_file = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/项目标签/项目标签_gemini-2.5-flash.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def check_missing_tags(csv_file_path, tags_json_path):\n",
    "    \"\"\"\n",
    "    检查CSV中的标签是否都在翻译字典中，打印缺失的标签\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path: CSV文件路径\n",
    "        tags_json_path: 翻译字典JSON文件路径\n",
    "    \"\"\"\n",
    "    \n",
    "    # 读取翻译字典\n",
    "    with open(tags_json_path, 'r', encoding='utf-8') as f:\n",
    "        translation_dict = json.load(f)[0]\n",
    "    \n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(csv_file_path, encoding='utf-8-sig')\n",
    "    \n",
    "    # 收集所有unique标签\n",
    "    all_tags = set()\n",
    "    for tags_str in df['项目标签（英文）']:\n",
    "        if pd.notna(tags_str) and tags_str != '' and tags_str != '需要额外确认':\n",
    "            tag_list = [tag.strip() for tag in str(tags_str).split(',')]\n",
    "            all_tags.update(tag_list)\n",
    "    \n",
    "    # 找出不在翻译字典中的标签\n",
    "    missing_tags = [tag for tag in all_tags if tag not in translation_dict and tag != '']\n",
    "    \n",
    "    if missing_tags:\n",
    "        print(\"以下标签不存在于翻译字典中，请添加中英文翻译，并返回一个英文：中文的字典：\")\n",
    "        for tag in sorted(missing_tags):\n",
    "            print(f\"  - {tag}\")\n",
    "\n",
    "# 使用示例\n",
    "check_missing_tags(\n",
    " csv_file, json_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载翻译字典，包含 323 个翻译条目\n",
      "成功读取CSV文件，包含 121 行数据\n",
      "已将'项目标签'列重命名为'项目标签（英文）'\n",
      "正在翻译项目标签...\n",
      "成功保存双语标签CSV文件到: /Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/项目标签/项目标签_gemini-2.5-flash.csv\n",
      "\n",
      "=== 处理统计 ===\n",
      "总记录数: 121\n",
      "有英文标签的记录: 116 (95.9%)\n",
      "有中文标签的记录: 116 (95.9%)\n",
      "\n",
      "=== 前5条记录示例 ===\n",
      "                               大学英文名称                                                                                专业英文名称                                                                                             项目标签（英文）                    项目标签（中文）\n",
      "Massachusetts Institute of Technology Civil and Environmental Engineering：Data Science for Engineering Systems (DSES) track Civil Engineering, Environmental Engineering, Interdisciplinary, Data Science, Computational Science 土木工程, 环境工程, 跨学科, 数据科学, 计算科学\n",
      "Massachusetts Institute of Technology                                                        Social and Engineering Systems                            Interdisciplinary, Data Science, Statistics, Engineering, Social Sciences   跨学科, 数据科学, 统计学, 工程学, 社会科学\n",
      "Massachusetts Institute of Technology                                               Data, Economics, and Development Policy                              Data Science, Economics, Policy, Development Studies, Interdisciplinary    数据科学, 经济学, 政策, 发展研究, 跨学科\n",
      "                   Harvard University                                                               Master in Public Policy              Data Science, Quantitative Analysis, Research Methods, Public Policy, Interdisciplinary 数据科学, 定量分析, 研究方法, 公共政策, 跨学科\n",
      "                   Harvard University                          Master in Public Administration in International Development              Quantitative Analysis, Data Science, Research Methods, Public Policy, Interdisciplinary 定量分析, 数据科学, 研究方法, 公共政策, 跨学科\n",
      "\n",
      "=== 需要额外确认的记录 (5 条) ===\n",
      "- Stanford University: ‌Education Data Science\n",
      "- Stanford University: Education Data Science (MS)\n",
      "- Stanford University: Biomedical Data Science\n",
      "- University of Michigan, Ann Arbor: Survey and Data Science\n",
      "- University of Washington: Earth and Space Sciences (PhD) ： DATA SCIENCE  TRACK\n",
      "\n",
      "✅ 项目标签双语化处理完成！\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def process_project_tags_bilingual(csv_file_path, tags_json_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    处理项目标签，将英文标签转换为中英双语标签\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path: 输入CSV文件路径\n",
    "        tags_json_path: 英中翻译字典JSON文件路径\n",
    "        output_csv_path: 输出CSV文件路径\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: 处理后的数据\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 读取翻译字典\n",
    "    try:\n",
    "        with open(tags_json_path, 'r', encoding='utf-8') as f:\n",
    "            translation_dict = json.load(f)[0]  # JSON文件中是一个包含字典的数组\n",
    "        print(f\"成功加载翻译字典，包含 {len(translation_dict)} 个翻译条目\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取翻译字典失败: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. 读取CSV文件\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path, encoding='utf-8-sig')\n",
    "        print(f\"成功读取CSV文件，包含 {len(df)} 行数据\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取CSV文件失败: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 3. 检查必要的列是否存在\n",
    "    if '项目标签' not in df.columns:\n",
    "        print(\"错误：CSV文件中未找到'项目标签'列\")\n",
    "        return None\n",
    "    \n",
    "    # 4. 重命名现有的项目标签列\n",
    "    df = df.rename(columns={'项目标签': '项目标签（英文）'})\n",
    "    print(\"已将'项目标签'列重命名为'项目标签（英文）'\")\n",
    "    \n",
    "    # 5. 创建翻译函数\n",
    "    def translate_tags(english_tags):\n",
    "        \"\"\"\n",
    "        将英文标签字符串转换为中文标签字符串\n",
    "        \n",
    "        Args:\n",
    "            english_tags: 英文标签字符串，用逗号分隔\n",
    "            \n",
    "        Returns:\n",
    "            str: 中文标签字符串，用逗号分隔\n",
    "        \"\"\"\n",
    "        if pd.isna(english_tags) or english_tags == '' or english_tags == '需要额外确认':\n",
    "            return english_tags  # 保持原值\n",
    "        \n",
    "        # 分割标签并清理空白\n",
    "        tag_list = [tag.strip() for tag in str(english_tags).split(',')]\n",
    "        translated_tags = []\n",
    "        untranslated_tags = []\n",
    "        \n",
    "        for tag in tag_list:\n",
    "            if tag in translation_dict:\n",
    "                translated_tags.append(translation_dict[tag])\n",
    "            else:\n",
    "                # 如果没有找到翻译，保留原英文\n",
    "                translated_tags.append(tag)\n",
    "                if tag != '需要额外确认' and tag != '':\n",
    "                    untranslated_tags.append(tag)\n",
    "        \n",
    "        # 如果有未翻译的标签，记录下来\n",
    "        if untranslated_tags:\n",
    "            print(f\"未找到翻译的标签: {untranslated_tags}\")\n",
    "        \n",
    "        return ', '.join(translated_tags)\n",
    "    \n",
    "    # 6. 应用翻译函数创建中文标签列\n",
    "    print(\"正在翻译项目标签...\")\n",
    "    df['项目标签（中文）'] = df['项目标签（英文）'].apply(translate_tags)\n",
    "    \n",
    "    # 7. 重新排列列的顺序，将双语标签列放在一起\n",
    "    # 找到原来项目标签列的位置\n",
    "    columns = df.columns.tolist()\n",
    "    english_tag_index = columns.index('项目标签（英文）')\n",
    "    \n",
    "    # 重新排列列顺序\n",
    "    new_columns = (columns[:english_tag_index] + \n",
    "                  ['项目标签（英文）', '项目标签（中文）'] + \n",
    "                  columns[english_tag_index+1:])\n",
    "    \n",
    "    # 移除重复的'项目标签（中文）'列（如果有的话）\n",
    "    new_columns = [col for i, col in enumerate(new_columns) \n",
    "                   if col != '项目标签（中文）' or i == english_tag_index + 1]\n",
    "    \n",
    "    df = df[new_columns]\n",
    "    \n",
    "    # 8. 创建输出目录（如果不存在）\n",
    "    output_dir = os.path.dirname(output_csv_path)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"创建输出目录: {output_dir}\")\n",
    "    \n",
    "    # 9. 保存处理后的CSV文件\n",
    "    try:\n",
    "        df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"成功保存双语标签CSV文件到: {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"保存CSV文件失败: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 10. 打印统计信息\n",
    "    print(\"\\n=== 处理统计 ===\")\n",
    "    total_records = len(df)\n",
    "    english_filled = len(df[df['项目标签（英文）'].notna() & \n",
    "                           (df['项目标签（英文）'] != '') & \n",
    "                           (df['项目标签（英文）'] != '需要额外确认')])\n",
    "    chinese_filled = len(df[df['项目标签（中文）'].notna() & \n",
    "                           (df['项目标签（中文）'] != '') & \n",
    "                           (df['项目标签（中文）'] != '需要额外确认')])\n",
    "    \n",
    "    print(f\"总记录数: {total_records}\")\n",
    "    print(f\"有英文标签的记录: {english_filled} ({english_filled/total_records:.1%})\")\n",
    "    print(f\"有中文标签的记录: {chinese_filled} ({chinese_filled/total_records:.1%})\")\n",
    "    \n",
    "    # 11. 显示前几条记录作为示例\n",
    "    print(\"\\n=== 前5条记录示例 ===\")\n",
    "    display_columns = ['大学英文名称', '专业英文名称', '项目标签（英文）', '项目标签（中文）']\n",
    "    available_columns = [col for col in display_columns if col in df.columns]\n",
    "    print(df[available_columns].head().to_string(index=False))\n",
    "    \n",
    "    # 12. 检查是否有需要额外确认的记录\n",
    "    need_confirmation = df[df['项目标签（英文）'] == '需要额外确认']\n",
    "    if len(need_confirmation) > 0:\n",
    "        print(f\"\\n=== 需要额外确认的记录 ({len(need_confirmation)} 条) ===\")\n",
    "        for _, row in need_confirmation.iterrows():\n",
    "            print(f\"- {row['大学英文名称']}: {row['专业英文名称']}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    # 执行处理\n",
    "    result_df = process_project_tags_bilingual(csv_file, json_file, output_file)\n",
    "    \n",
    "    if result_df is not None:\n",
    "        print(\"\\n✅ 项目标签双语化处理完成！\")\n",
    "    else:\n",
    "        print(\"\\n❌ 处理失败，请检查错误信息\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
