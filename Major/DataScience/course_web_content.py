import re
import requests
from bs4 import BeautifulSoup
from collections import Counter

def analyze_curriculum_content_enhanced(url_or_content, is_url=True):
    """
    å¢å¼ºç‰ˆè¯¾ç¨‹å†…å®¹è¯†åˆ« - ä¸“ä¸ºè‹±æ–‡ç½‘é¡µå’Œå¤šå­¦ç§‘è®¾è®¡
    """
    
    # è·å–å†…å®¹
    if is_url:
        try:
            response = requests.get(url_or_content, timeout=10, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            if response.status_code != 200:
                return {"is_curriculum": False, "reason": f"HTTP {response.status_code}"}
            html_content = response.text
            url = url_or_content
        except Exception as e:
            return {"is_curriculum": False, "reason": f"Failed to fetch: {str(e)}"}
    else:
        html_content = url_or_content
        url = "N/A"
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ç§»é™¤ä¸ç›¸å…³æ ‡ç­¾
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()
        
        text = soup.get_text().lower()
        original_text = soup.get_text()  # ä¿ç•™åŸå§‹å¤§å°å†™ç”¨äºè¯¾ç¨‹ç¼–å·åŒ¹é…
        
        analysis = {
            "url": url,
            "is_curriculum": False,
            "confidence_score": 0,
            "details": {},
            "course_codes_found": [],
            "subject_areas": [],
            "reasons": []
        }
        
        # === 1. è¶…å¼ºæŒ‡ç¤ºè¯ (40åˆ†/ä¸ª) ===
        ultra_strong_indicators = [
            'degree requirements', 'graduation requirements', 'curriculum overview',
            'course requirements', 'academic requirements', 'program requirements',
            'course catalog', 'course listings', 'required coursework',
            'core curriculum', 'curriculum structure', 'degree plan'
        ]
        
        ultra_matches = sum(1 for indicator in ultra_strong_indicators if indicator in text)
        if ultra_matches > 0:
            analysis["confidence_score"] += ultra_matches * 40
            analysis["reasons"].append(f"Found {ultra_matches} ultra-strong curriculum indicators")
        
        # === 2. å¼ºæŒ‡ç¤ºè¯ (25åˆ†/ä¸ª) ===
        strong_indicators = [
            'course list', 'course sequence', 'required courses', 'elective courses',
            'core courses', 'foundation courses', 'prerequisite', 'prerequisites',
            'course descriptions', 'syllabus', 'credit hours', 'credit requirements',
            'semester plan', 'academic plan', 'study plan'
        ]
        
        strong_matches = sum(1 for indicator in strong_indicators if indicator in text)
        if strong_matches > 0:
            analysis["confidence_score"] += strong_matches * 25
            analysis["reasons"].append(f"Found {strong_matches} strong curriculum indicators")
        
        # === 3. è¯¦ç»†çš„å­¦ç§‘ç¼–ç æ¨¡å¼ ===
        subject_codes = {
            # è®¡ç®—æœº/æ•°æ®ç§‘å­¦
            'cs_data': ['CS', 'CSE', 'COMP', 'CMSC', 'CIS', 'INFO', 'INFOSCI', 'DATA', 'DSCI', 'STAT', 'STATS'],
            # æ•°å­¦/ç»Ÿè®¡
            'math_stat': ['MATH', 'STAT', 'STATS', 'APPL', 'PROB', 'STOC'],
            # ç¤¾ä¼šç§‘å­¦
            'social_sci': ['PSYC', 'PSYCH', 'SOC', 'SOCI', 'ANTH', 'ANTHRO', 'POLI', 'POLISCI', 'GOVT', 'POLS'],
            # ç»æµ/å•†ç§‘
            'econ_business': ['ECON', 'ECON', 'BUS', 'BUSN', 'MGMT', 'MGTM', 'FIN', 'FINC', 'ACCT', 'MARK', 'MKTG'],
            # æ•™è‚²
            'education': ['EDUC', 'ED', 'TEACH', 'CURR', 'INST', 'LING', 'APPL'],
            # ä¼ åª’/æ–°é—»
            'media_comm': ['COMM', 'CMCL', 'JOUR', 'JRNL', 'MEDIA', 'FILM', 'DIGT'],
            # äººæ–‡å­¦ç§‘
            'humanities': ['ENGL', 'HIST', 'PHIL', 'RELI', 'CLAS', 'FREN', 'SPAN', 'GERM'],
            # å·¥ç¨‹
            'engineering': ['ENGR', 'MECH', 'ELEC', 'CHEM', 'CIVL', 'BIOE', 'AERO'],
            # åŒ»å­¦/ç”Ÿç‰©
            'bio_med': ['BIOL', 'CHEM', 'PHYS', 'MED', 'NURS', 'PUBH', 'EPID'],
            # è‰ºæœ¯/è®¾è®¡
            'arts': ['ART', 'ARTS', 'MUS', 'MUSC', 'THEA', 'DANC', 'DSGN']
        }
        
        # è¯¾ç¨‹ç¼–å·åŒ¹é…æ¨¡å¼ (æ›´ç²¾ç¡®)
        course_patterns = [
            # æ ‡å‡†æ ¼å¼: SUBJ 1234, SUBJ-1234, SUBJ1234
            r'\b([A-Z]{2,8})[\s\-]?(\d{3,4}[A-Z]?)\b',
            # å¸¦ç‚¹å·: SUBJ.1234
            r'\b([A-Z]{2,8})\.(\d{3,4}[A-Z]?)\b',
            # å¸¦å†’å·: SUBJ:1234
            r'\b([A-Z]{2,8}):(\d{3,4}[A-Z]?)\b',
            # é•¿æ ¼å¼: SUBJECT 1234
            r'\b([A-Z]{3,10})\s+(\d{3,4}[A-Z]?)\b'
        ]
        
        # æŸ¥æ‰¾è¯¾ç¨‹ç¼–å·
        all_course_codes = []
        subject_areas_found = set()
        
        for pattern in course_patterns:
            matches = re.findall(pattern, original_text)
            for subject, number in matches:
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å­¦ç§‘ä»£ç 
                for category, codes in subject_codes.items():
                    if subject in codes:
                        course_code = f"{subject} {number}"
                        all_course_codes.append(course_code)
                        subject_areas_found.add(category)
                        break
                else:
                    # å¦‚æœä¸åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­ï¼Œä½†ç¬¦åˆå¸¸è§æ¨¡å¼ï¼Œä¹Ÿå¯èƒ½æ˜¯è¯¾ç¨‹
                    if len(subject) >= 3 and subject.isalpha():
                        course_code = f"{subject} {number}"
                        all_course_codes.append(course_code)
        
        # å»é‡
        unique_courses = list(dict.fromkeys(all_course_codes))  # ä¿æŒé¡ºåºçš„å»é‡
        analysis["course_codes_found"] = unique_courses[:15]  # æœ€å¤šæ˜¾ç¤º15ä¸ª
        analysis["subject_areas"] = list(subject_areas_found)
        analysis["details"]["course_count"] = len(unique_courses)
        
        # è¯¾ç¨‹ç¼–å·è¯„åˆ†
        if len(unique_courses) >= 10:
            analysis["confidence_score"] += 50
            analysis["reasons"].append(f"Found {len(unique_courses)} course codes (excellent)")
        elif len(unique_courses) >= 5:
            analysis["confidence_score"] += 35
            analysis["reasons"].append(f"Found {len(unique_courses)} course codes (good)")
        elif len(unique_courses) >= 2:
            analysis["confidence_score"] += 20
            analysis["reasons"].append(f"Found {len(unique_courses)} course codes")
        
        # === 4. å­¦åˆ†/å­¦æ—¶æ¨¡å¼è¯†åˆ« ===
        credit_patterns = [
            r'(\d+)\s*credit[s]?\s*hour[s]?',
            r'(\d+)\s*credit[s]?',
            r'(\d+)\s*unit[s]?',
            r'(\d+)\s*hour[s]?\s*per\s*week',
            r'\((\d+)\s*cr\)',
            r'\((\d+)\s*credits?\)',
            r'(\d+)-credit',
            r'total\s*of\s*(\d+)\s*credit'
        ]
        
        credit_mentions = []
        for pattern in credit_patterns:
            matches = re.findall(pattern, text)
            credit_mentions.extend(matches)
        
        analysis["details"]["credit_mentions"] = len(credit_mentions)
        if len(credit_mentions) >= 5:
            analysis["confidence_score"] += 25
            analysis["reasons"].append(f"Found {len(credit_mentions)} credit/hour mentions")
        elif len(credit_mentions) >= 2:
            analysis["confidence_score"] += 15
            analysis["reasons"].append(f"Found {len(credit_mentions)} credit mentions")
        
        # === 5. è¯¾ç¨‹æè¿°å…³é”®è¯ ===
        course_keywords = [
            'prerequisite', 'corequisite', 'enrollment', 'instructor', 'semester',
            'quarter', 'academic year', 'course load', 'full-time', 'part-time',
            'thesis', 'dissertation', 'capstone', 'internship', 'practicum',
            'seminar', 'workshop', 'laboratory', 'lecture', 'tutorial'
        ]
        
        keyword_matches = sum(1 for keyword in course_keywords if keyword in text)
        analysis["details"]["keyword_matches"] = keyword_matches
        if keyword_matches >= 5:
            analysis["confidence_score"] += 20
            analysis["reasons"].append(f"Found {keyword_matches} course-related keywords")
        elif keyword_matches >= 3:
            analysis["confidence_score"] += 10
            analysis["reasons"].append(f"Found {keyword_matches} course keywords")
        
        # === 6. å­¦ä½å±‚æ¬¡è¯†åˆ« ===
        degree_patterns = [
            r'master[\'s]?\s*degree', r'doctoral\s*degree', r'phd\s*degree',
            r'bachelor[\'s]?\s*degree', r'undergraduate', r'graduate',
            r'm\.?s\.?', r'm\.?a\.?', r'ph\.?d\.?', r'b\.?s\.?', r'b\.?a\.?'
        ]
        
        degree_mentions = sum(1 for pattern in degree_patterns if re.search(pattern, text))
        if degree_mentions >= 2:
            analysis["confidence_score"] += 10
            analysis["reasons"].append("Found degree-level indicators")
        
        # === 7. HTMLç»“æ„åˆ†æ ===
        # æŸ¥æ‰¾è¯¾ç¨‹åˆ—è¡¨ç»“æ„
        course_list_indicators = 0
        
        # æ£€æŸ¥è¡¨æ ¼
        tables = soup.find_all('table')
        for table in tables:
            table_text = table.get_text().lower()
            if any(word in table_text for word in ['course', 'credit', 'semester', 'requirement']):
                course_list_indicators += 10
        
        # æ£€æŸ¥åˆ—è¡¨
        lists = soup.find_all(['ul', 'ol'])
        for lst in lists:
            list_text = lst.get_text().lower()
            if any(word in list_text for word in ['course', 'credit', 'requirement']) and len(lst.find_all('li')) >= 3:
                course_list_indicators += 5
        
        # æ£€æŸ¥æ ‡é¢˜ç»“æ„
        headers = soup.find_all(['h1', 'h2', 'h3', 'h4'])
        for header in headers:
            header_text = header.get_text().lower()
            if any(word in header_text for word in ['curriculum', 'course', 'requirement', 'program']):
                course_list_indicators += 3
        
        if course_list_indicators > 0:
            analysis["confidence_score"] += min(course_list_indicators, 25)
            analysis["reasons"].append("Found structured curriculum content")
        
        # === 8. è´Ÿé¢æŒ‡æ ‡æ£€æµ‹ ===
        negative_indicators = [
            'news', 'blog', 'event', 'calendar', 'faculty directory',
            'contact us', 'about us', 'homepage', 'main page',
            'research projects', 'publications', 'biography'
        ]
        
        negative_count = sum(1 for indicator in negative_indicators if indicator in text)
        if negative_count >= 3:
            analysis["confidence_score"] -= 15
            analysis["reasons"].append("Contains non-curriculum content indicators")
        
        # === æœ€ç»ˆåˆ¤æ–­ ===
        # åŠ¨æ€é˜ˆå€¼ï¼šå¦‚æœå‘ç°å¾ˆå¤šè¯¾ç¨‹ç¼–å·ï¼Œé™ä½æ€»ä½“è¦æ±‚
        if len(unique_courses) >= 8:
            threshold = 40
        elif len(unique_courses) >= 4:
            threshold = 50
        else:
            threshold = 60
        
        analysis["is_curriculum"] = analysis["confidence_score"] >= threshold
        analysis["details"]["threshold_used"] = threshold
        
        # ç½®ä¿¡åº¦ç­‰çº§
        if analysis["confidence_score"] >= 80:
            analysis["confidence_level"] = "Very High"
        elif analysis["confidence_score"] >= 60:
            analysis["confidence_level"] = "High"
        elif analysis["confidence_score"] >= 40:
            analysis["confidence_level"] = "Medium"
        elif analysis["confidence_score"] >= 20:
            analysis["confidence_level"] = "Low"
        else:
            analysis["confidence_level"] = "Very Low"
        
        return analysis
        
    except Exception as e:
        return {"is_curriculum": False, "reason": f"Analysis failed: {str(e)}"}

def batch_analyze_enhanced(urls):
    """å¢å¼ºç‰ˆæ‰¹é‡åˆ†æ"""
    results = []
    
    print("ğŸ“ å¼€å§‹æ‰¹é‡è¯¾ç¨‹é¡µé¢åˆ†æ...")
    print("=" * 60)
    
    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] ğŸ” åˆ†æ: {url}")
        result = analyze_curriculum_content_enhanced(url)
        results.append(result)
        
        if result["is_curriculum"]:
            print(f"âœ… **è¯¾ç¨‹é¡µé¢** (ç½®ä¿¡åº¦: {result['confidence_score']}, ç­‰çº§: {result.get('confidence_level', 'N/A')})")
            if result.get("course_codes_found"):
                print(f"   ğŸ“š è¯¾ç¨‹ç¼–å·ç¤ºä¾‹: {', '.join(result['course_codes_found'][:5])}")
            if result.get("subject_areas"):
                print(f"   ğŸ¯ å­¦ç§‘é¢†åŸŸ: {', '.join(result['subject_areas'])}")
        else:
            print(f"âŒ éè¯¾ç¨‹é¡µé¢ (ç½®ä¿¡åº¦: {result['confidence_score']}, ç­‰çº§: {result.get('confidence_level', 'N/A')})")
        
        if result.get("reasons"):
            print(f"   ğŸ’¡ åˆ¤æ–­ä¾æ®: {'; '.join(result['reasons'][:3])}")  # åªæ˜¾ç¤ºå‰3ä¸ªåŸå› 
        
        print("-" * 40)
    
    # ç»Ÿè®¡æ‘˜è¦
    curriculum_count = sum(1 for r in results if r["is_curriculum"])
    print(f"\nğŸ“Š åˆ†æå®Œæˆ:")
    print(f"   - æ€»è®¡åˆ†æ: {len(results)} ä¸ªURL")
    print(f"   - è¯†åˆ«ä¸ºè¯¾ç¨‹é¡µé¢: {curriculum_count} ä¸ª")
    print(f"   - è¯†åˆ«ç‡: {curriculum_count/len(results)*100:.1f}%")
    
    return results
