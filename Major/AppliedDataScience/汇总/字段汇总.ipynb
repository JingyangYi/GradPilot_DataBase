{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主CSV文件路径\n",
    "main_csv_path = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/AppliedDataScience_df1.csv\"\n",
    "\n",
    "# 基础路径\n",
    "base_path = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records\"\n",
    "\n",
    "# 保存结果\n",
    "output_dir = \"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/汇总\"\n",
    "\n",
    "# 输出csv名字\n",
    "merged_file_name = \"美研-应用数据科学\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 读取主数据集...\n",
      "主数据集包含 121 个项目\n",
      "\n",
      "🔄 开始添加字段数据...\n",
      "============================================================\n",
      "\n",
      "📁 处理文件夹: 更多项目信息\n",
      "🎯 添加字段: ['更多项目信息']\n",
      "   📄 发现CSV文件: 更多项目信息_提取结果.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 121 条记录\n",
      "   ✅ 行数匹配: 121\n",
      "   ➕ 添加字段: ['更多项目信息']\n",
      "     📝 添加: 更多项目信息\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 更多项目信息: 110/121\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 课程网址\n",
      "🎯 添加字段: ['课程网址']\n",
      "   📄 发现CSV文件: 课程网址_规则处理结果.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 121 条记录\n",
      "   ✅ 行数匹配: 121\n",
      "   ➕ 添加字段: ['课程网址']\n",
      "     📝 添加: 课程网址\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 课程网址: 62/121\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 面试\n",
      "🎯 添加字段: ['面试']\n",
      "   📄 发现CSV文件: 面试需求_处理结果_newp.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 121 条记录\n",
      "   ✅ 行数匹配: 121\n",
      "   ➕ 添加字段: ['面试']\n",
      "     📝 添加: 面试\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 面试: 91/121\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 申请费减免\n",
      "🎯 添加字段: ['申请费减免']\n",
      "   📄 发现CSV文件: 申请费减免_gemini-2.5-flash_0_121_processed.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 121 条记录\n",
      "   ✅ 行数匹配: 121\n",
      "   ➕ 添加字段: ['申请费减免']\n",
      "     📝 添加: 申请费减免\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 申请费减免: 105/121\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 所属院系（英文）\n",
      "🎯 添加字段: ['所属院系英文名称', '所属院系网址']\n",
      "   📄 发现CSV文件: 所属院系英文_处理结果.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 120 条记录\n",
      "   ⚠️  警告: 行数不匹配! 主数据集: 121, 字段文件: 120\n",
      "   📏 使用最小长度: 120\n",
      "   ⚠️  缺失字段: ['所属院系英文名称']\n",
      "   📋 可用字段: ['大学英文名称', '学位', '专业英文名称', '所属院系', '所属院系（英文）', '所属院系网址', '招生网址', '专业网址', 'valid_response_count', 'consistency_type', 'consistency_detail', 'response_1', 'response_1_valid', 'response_2', 'response_2_valid', 'response_3', 'response_3_valid']\n",
      "   ➕ 添加字段: ['所属院系网址']\n",
      "     📝 所属院系网址 → 所属院系网址_1 (重命名避免冲突)\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 所属院系网址_1: 118/121\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 推荐信\n",
      "🎯 添加字段: ['推荐信']\n",
      "   📄 发现CSV文件: 推荐信_gemini-2.5-flash_0_121_processed.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 121 条记录\n",
      "   ✅ 行数匹配: 121\n",
      "   ➕ 添加字段: ['推荐信']\n",
      "     📝 添加: 推荐信\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 推荐信: 120/121\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 托福送分ETS code\n",
      "🎯 添加字段: ['托福送分ETS code']\n",
      "   📄 发现CSV文件: 托福送分ETS code_gemini-2.0-flash_0_121_processed.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 121 条记录\n",
      "   ✅ 行数匹配: 121\n",
      "   ➕ 添加字段: ['托福送分ETS code']\n",
      "     📝 添加: 托福送分ETS code\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 托福送分ETS code: 121/121\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 项目标签\n",
      "🎯 添加字段: ['项目标签（英文）', '项目标签（中文）']\n",
      "   📄 发现CSV文件: 项目标签_gemini-2.5-flash.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 121 条记录\n",
      "   ✅ 行数匹配: 121\n",
      "   ➕ 添加字段: ['项目标签（英文）', '项目标签（中文）']\n",
      "     📝 添加: 项目标签（英文）\n",
      "     📝 添加: 项目标签（中文）\n",
      "   ✅ 成功添加 2 个字段\n",
      "   📊 非空值统计: 项目标签（英文）: 121/121, 项目标签（中文）: 121/121\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: 职业项目\n",
      "🎯 添加字段: ['职业项目']\n",
      "   📄 发现CSV文件: 职业项目.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 121 条记录\n",
      "   ✅ 行数匹配: 121\n",
      "   ➕ 添加字段: ['职业项目']\n",
      "     📝 添加: 职业项目\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: 职业项目: 121/121\n",
      "----------------------------------------\n",
      "\n",
      "📁 处理文件夹: Capstone或Thesis\n",
      "🎯 添加字段: ['Gemini最终毕业要求']\n",
      "   ⚠️  警告: 文件夹中发现多个CSV文件，使用第一个: ['/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/Capstone或Thesis/Capstone或Thesis.csv', '/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/fields_records/Capstone或Thesis/Capstone或Thesis_修复版.csv']\n",
      "   📋 所有CSV文件: ['Capstone或Thesis.csv', 'Capstone或Thesis_修复版.csv']\n",
      "   📄 发现CSV文件: Capstone或Thesis.csv\n",
      "   📖 读取文件...\n",
      "   📊 找到 121 条记录\n",
      "   ✅ 行数匹配: 121\n",
      "   ➕ 添加字段: ['Gemini最终毕业要求']\n",
      "     📝 添加: Gemini最终毕业要求\n",
      "   ✅ 成功添加 1 个字段\n",
      "   📊 非空值统计: Gemini最终毕业要求: 121/121\n",
      "----------------------------------------\n",
      "\n",
      "💾 保存整合结果...\n",
      "✅ 文件已保存至: /Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/AppliedDataScience/汇总/美研-应用数据科学.csv\n",
      "\n",
      "📊 整合完成统计:\n",
      "============================================================\n",
      "总字段组数: 10\n",
      "成功添加: 10\n",
      "添加失败: 0\n",
      "最终数据集大小: 121 行 × 35 列\n",
      "\n",
      "📋 详细处理结果:\n",
      "✅ 更多项目信息:\n",
      "     CSV文件: 更多项目信息_提取结果.csv\n",
      "     记录数: 121\n",
      "     添加的列: ['更多项目信息']\n",
      "✅ 课程网址:\n",
      "     CSV文件: 课程网址_规则处理结果.csv\n",
      "     记录数: 121\n",
      "     添加的列: ['课程网址']\n",
      "✅ 面试:\n",
      "     CSV文件: 面试需求_处理结果_newp.csv\n",
      "     记录数: 121\n",
      "     添加的列: ['面试']\n",
      "✅ 申请费减免:\n",
      "     CSV文件: 申请费减免_gemini-2.5-flash_0_121_processed.csv\n",
      "     记录数: 121\n",
      "     添加的列: ['申请费减免']\n",
      "✅ 所属院系（英文）:\n",
      "     CSV文件: 所属院系英文_处理结果.csv\n",
      "     记录数: 120\n",
      "     添加的列: ['所属院系网址_1']\n",
      "✅ 推荐信:\n",
      "     CSV文件: 推荐信_gemini-2.5-flash_0_121_processed.csv\n",
      "     记录数: 121\n",
      "     添加的列: ['推荐信']\n",
      "✅ 托福送分ETS code:\n",
      "     CSV文件: 托福送分ETS code_gemini-2.0-flash_0_121_processed.csv\n",
      "     记录数: 121\n",
      "     添加的列: ['托福送分ETS code']\n",
      "✅ 项目标签:\n",
      "     CSV文件: 项目标签_gemini-2.5-flash.csv\n",
      "     记录数: 121\n",
      "     添加的列: ['项目标签（英文）', '项目标签（中文）']\n",
      "✅ 职业项目:\n",
      "     CSV文件: 职业项目.csv\n",
      "     记录数: 121\n",
      "     添加的列: ['职业项目']\n",
      "✅ Capstone或Thesis:\n",
      "     CSV文件: Capstone或Thesis.csv\n",
      "     记录数: 121\n",
      "     添加的列: ['Gemini最终毕业要求']\n",
      "\n",
      "🔍 最终数据质量检查:\n",
      "数据集形状: (121, 35)\n",
      "列名: ['排名', '大学名称', '大学英文名称', '所在城市', '专业中文名称', '专业英文名称', '所属院系', '所属院系（英文）', '所属院系网址', '学位', '专业领域', '课程长度', '申请费（美元)', '开学期', '截止日期', 'GPA', '托福', '雅思', 'GRE', 'GMAT', '学术背景', '材料要求', '招生网址', '专业网址', '更多项目信息', '课程网址', '面试', '申请费减免', '所属院系网址_1', '推荐信', '托福送分ETS code', '项目标签（英文）', '项目标签（中文）', '职业项目', 'Gemini最终毕业要求']\n",
      "\n",
      "📈 各字段覆盖率:\n",
      "   排名: 121/121 (100.0%)\n",
      "   所属院系（英文）: 118/121 (97.5%)\n",
      "   所属院系网址: 118/121 (97.5%)\n",
      "   更多项目信息: 110/121 (90.9%)\n",
      "   课程网址: 62/121 (51.2%)\n",
      "   面试: 91/121 (75.2%)\n",
      "   申请费减免: 105/121 (86.8%)\n",
      "   所属院系网址_1: 118/121 (97.5%)\n",
      "   推荐信: 120/121 (99.2%)\n",
      "   托福送分ETS code: 121/121 (100.0%)\n",
      "   项目标签（英文）: 121/121 (100.0%)\n",
      "   项目标签（中文）: 121/121 (100.0%)\n",
      "   职业项目: 121/121 (100.0%)\n",
      "   Gemini最终毕业要求: 121/121 (100.0%)\n",
      "\n",
      "🎉 整合完成! 最终数据集形状: (121, 35)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def integrate_fields_to_main_dataset():\n",
    "    \"\"\"\n",
    "    将各个字段文件夹中的CSV数据按行索引直接添加到主数据集中\n",
    "    自动发现每个文件夹中的CSV文件（假设每个文件夹只有一个CSV文件）\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: 整合后的数据集\n",
    "    \"\"\"\n",
    "    \n",
    "    # 读取主数据集\n",
    "    print(\"📖 读取主数据集...\")\n",
    "    main_df = pd.read_csv(main_csv_path, encoding='utf-8-sig')\n",
    "    print(f\"主数据集包含 {len(main_df)} 个项目\")\n",
    "    \n",
    "    # 字段映射配置：文件夹名 -> 要提取的字段列表（不再需要指定CSV文件名）\n",
    "    field_configs = {\n",
    "        '更多项目信息': ['更多项目信息'],\n",
    "        '课程网址': ['课程网址'],\n",
    "        '面试': ['面试'],\n",
    "        # '其他字段': 跳过\n",
    "        '申请费减免': ['申请费减免'],\n",
    "        '所属院系（英文）': ['所属院系英文名称', '所属院系网址'],\n",
    "        '推荐信': ['推荐信'],\n",
    "        '托福送分ETS code': ['托福送分ETS code'],\n",
    "        '项目标签': ['项目标签（英文）', '项目标签（中文）'],\n",
    "        '职业项目': ['职业项目'],\n",
    "        'Capstone或Thesis': ['毕业要求']\n",
    "    }\n",
    "    \n",
    "    # 记录处理统计\n",
    "    processing_stats = {\n",
    "        'total_fields': len(field_configs),\n",
    "        'successful_adds': 0,\n",
    "        'failed_adds': 0,\n",
    "        'field_details': {}\n",
    "    }\n",
    "    \n",
    "    print(\"\\n🔄 开始添加字段数据...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 逐个处理每个字段\n",
    "    for folder_name, target_columns in field_configs.items():\n",
    "        print(f\"\\n📁 处理文件夹: {folder_name}\")\n",
    "        print(f\"🎯 添加字段: {target_columns}\")\n",
    "        \n",
    "        # 构建文件夹路径\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        \n",
    "        field_stats = {\n",
    "            'folder_path': folder_path,\n",
    "            'target_columns': target_columns,\n",
    "            'folder_exists': False,\n",
    "            'csv_found': False,\n",
    "            'csv_path': None,\n",
    "            'records_count': 0,\n",
    "            'columns_added': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 检查文件夹是否存在\n",
    "            if not os.path.exists(folder_path):\n",
    "                print(f\"   ❌ 文件夹不存在: {folder_path}\")\n",
    "                field_stats['error'] = \"文件夹不存在\"\n",
    "                processing_stats['failed_adds'] += 1\n",
    "                processing_stats['field_details'][folder_name] = field_stats\n",
    "                continue\n",
    "                \n",
    "            field_stats['folder_exists'] = True\n",
    "            \n",
    "            # 自动发现文件夹中的CSV文件\n",
    "            csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "            \n",
    "            if not csv_files:\n",
    "                print(f\"   ❌ 文件夹中没有找到CSV文件: {folder_path}\")\n",
    "                field_stats['error'] = \"没有找到CSV文件\"\n",
    "                processing_stats['failed_adds'] += 1\n",
    "                processing_stats['field_details'][folder_name] = field_stats\n",
    "                continue\n",
    "            \n",
    "            if len(csv_files) > 1:\n",
    "                print(f\"   ⚠️  警告: 文件夹中发现多个CSV文件，使用第一个: {csv_files}\")\n",
    "                print(f\"   📋 所有CSV文件: {[os.path.basename(f) for f in csv_files]}\")\n",
    "            \n",
    "            # 使用第一个找到的CSV文件\n",
    "            csv_path = csv_files[0]\n",
    "            csv_filename = os.path.basename(csv_path)\n",
    "            field_stats['csv_found'] = True\n",
    "            field_stats['csv_path'] = csv_path\n",
    "            \n",
    "            print(f\"   📄 发现CSV文件: {csv_filename}\")\n",
    "            \n",
    "            # 读取字段CSV文件\n",
    "            print(f\"   📖 读取文件...\")\n",
    "            field_df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "            field_stats['records_count'] = len(field_df)\n",
    "            print(f\"   📊 找到 {len(field_df)} 条记录\")\n",
    "            \n",
    "            # 检查行数是否匹配\n",
    "            if len(field_df) != len(main_df):\n",
    "                print(f\"   ⚠️  警告: 行数不匹配! 主数据集: {len(main_df)}, 字段文件: {len(field_df)}\")\n",
    "                # 可以选择截断或填充，这里选择使用较短的长度\n",
    "                min_length = min(len(main_df), len(field_df))\n",
    "                print(f\"   📏 使用最小长度: {min_length}\")\n",
    "            else:\n",
    "                min_length = len(main_df)\n",
    "                print(f\"   ✅ 行数匹配: {min_length}\")\n",
    "            \n",
    "            # 检查所需列是否存在\n",
    "            available_columns = field_df.columns.tolist()\n",
    "            missing_columns = [col for col in target_columns if col not in available_columns]\n",
    "            \n",
    "            if missing_columns:\n",
    "                print(f\"   ⚠️  缺失字段: {missing_columns}\")\n",
    "                print(f\"   📋 可用字段: {available_columns}\")\n",
    "                # 只使用存在的字段\n",
    "                existing_columns = [col for col in target_columns if col in available_columns]\n",
    "                if not existing_columns:\n",
    "                    print(f\"   ❌ 没有可用的目标字段\")\n",
    "                    field_stats['error'] = \"没有可用的目标字段\"\n",
    "                    processing_stats['failed_adds'] += 1\n",
    "                    processing_stats['field_details'][folder_name] = field_stats\n",
    "                    continue\n",
    "                target_columns = existing_columns\n",
    "            \n",
    "            # 直接添加字段到主数据集\n",
    "            print(f\"   ➕ 添加字段: {target_columns}\")\n",
    "            added_columns = []\n",
    "            \n",
    "            for column in target_columns:\n",
    "                if column in field_df.columns:\n",
    "                    # 确保不会覆盖已存在的同名列\n",
    "                    new_column_name = column\n",
    "                    counter = 1\n",
    "                    while new_column_name in main_df.columns:\n",
    "                        new_column_name = f\"{column}_{counter}\"\n",
    "                        counter += 1\n",
    "                    \n",
    "                    # 添加列数据（截断到最小长度）\n",
    "                    main_df[new_column_name] = field_df[column].iloc[:min_length].reset_index(drop=True)\n",
    "                    added_columns.append(new_column_name)\n",
    "                    \n",
    "                    if new_column_name != column:\n",
    "                        print(f\"     📝 {column} → {new_column_name} (重命名避免冲突)\")\n",
    "                    else:\n",
    "                        print(f\"     📝 添加: {column}\")\n",
    "            \n",
    "            field_stats['columns_added'] = added_columns\n",
    "            \n",
    "            # 统计非空值\n",
    "            non_null_stats = []\n",
    "            for col in added_columns:\n",
    "                non_null_count = main_df[col].notna().sum()\n",
    "                non_null_stats.append(f\"{col}: {non_null_count}/{len(main_df)}\")\n",
    "            \n",
    "            print(f\"   ✅ 成功添加 {len(added_columns)} 个字段\")\n",
    "            print(f\"   📊 非空值统计: {', '.join(non_null_stats)}\")\n",
    "            \n",
    "            processing_stats['successful_adds'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ 处理失败: {str(e)}\")\n",
    "            field_stats['error'] = str(e)\n",
    "            processing_stats['failed_adds'] += 1\n",
    "        \n",
    "        processing_stats['field_details'][folder_name] = field_stats\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"{merged_file_name}.csv\")\n",
    "    \n",
    "    print(f\"\\n💾 保存整合结果...\")\n",
    "    main_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ 文件已保存至: {output_path}\")\n",
    "    \n",
    "    # 输出处理统计\n",
    "    print(f\"\\n📊 整合完成统计:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"总字段组数: {processing_stats['total_fields']}\")\n",
    "    print(f\"成功添加: {processing_stats['successful_adds']}\")\n",
    "    print(f\"添加失败: {processing_stats['failed_adds']}\")\n",
    "    print(f\"最终数据集大小: {main_df.shape[0]} 行 × {main_df.shape[1]} 列\")\n",
    "    \n",
    "    # 详细统计\n",
    "    print(f\"\\n📋 详细处理结果:\")\n",
    "    for folder_name, stats in processing_stats['field_details'].items():\n",
    "        status = \"✅\" if 'error' not in stats else \"❌\"\n",
    "        print(f\"{status} {folder_name}:\")\n",
    "        if 'error' in stats:\n",
    "            print(f\"     错误: {stats['error']}\")\n",
    "        else:\n",
    "            print(f\"     CSV文件: {os.path.basename(stats['csv_path']) if stats['csv_path'] else 'N/A'}\")\n",
    "            print(f\"     记录数: {stats['records_count']}\")\n",
    "            print(f\"     添加的列: {stats['columns_added']}\")\n",
    "    \n",
    "    # 检查最终数据质量\n",
    "    print(f\"\\n🔍 最终数据质量检查:\")\n",
    "    print(f\"数据集形状: {main_df.shape}\")\n",
    "    print(f\"列名: {list(main_df.columns)}\")\n",
    "    \n",
    "    # 显示每列的非空值统计\n",
    "    print(f\"\\n📈 各字段覆盖率:\")\n",
    "    total_records = len(main_df)\n",
    "    \n",
    "    # 只显示新添加的字段的统计\n",
    "    original_columns = ['大学排名', '大学名称', '大学英文名称', '所在城市', '专业中文名称', '学位', \n",
    "                       '专业英文名称', '所属院系', '专业领域', '课程长度', '申请费（美元)', '开学期', \n",
    "                       '截止日期', 'GPA', '托福', '雅思', 'GRE', 'GMAT', '学术背景', '材料要求', \n",
    "                       '招生网址', '专业网址']\n",
    "    \n",
    "    new_columns = [col for col in main_df.columns if col not in original_columns]\n",
    "    \n",
    "    for column in new_columns:\n",
    "        non_null_count = main_df[column].notna().sum()\n",
    "        coverage = (non_null_count / total_records) * 100\n",
    "        print(f\"   {column}: {non_null_count}/{total_records} ({coverage:.1f}%)\")\n",
    "    \n",
    "    return main_df, processing_stats\n",
    "\n",
    "# 运行整合函数\n",
    "if __name__ == \"__main__\":\n",
    "    final_df, stats = integrate_fields_to_main_dataset()\n",
    "    print(f\"\\n🎉 整合完成! 最终数据集形状: {final_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
