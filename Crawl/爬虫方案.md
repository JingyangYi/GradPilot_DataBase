# 爬虫方案

---

## 1. 项目目标

建立一个可扩展的爬虫与数据管理体系，用于递归抓取研究生/博士项目的多层网页，并为后续 **字段自动填充**（正则/LLM）与 **人工校验** 提供统一的数据底座。

---

## 2. 整体流程（ETL Pipeline）

```
CSV(基础信息) ──► ① URL 发现 ──► ② 网页抓取 ──► ③ 信息抽取 ──► ④ 结果校验/回填 ──► 最终表
                 ▲                                                │
                 └────────────────────── 增量更新 ───────────────────┘
```

> **核心要点**：每一步可以独立迭代、并行扩缩容；所有原始数据都可追溯。

---

## 3. 数据模型与存储要求

### 3.1 树模型（PageNode）

```
Project ID
  └── Root URL
        ├── Child URL 1
        │     ├── Child URL 1.1
        │     └── Child URL 1.2
        └── Child URL 2
```

### 3.2 PageNode 数据结构（Python 示例）

```python
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any
from datetime import datetime

@dataclass
class CrawlMetadata:
    """爬虫元数据，用于版本控制和追溯"""
    crawl_run_id: str
    crawl_time: str  # ISO 8601
    crawler_version: str
    max_depth: int
    success: bool
    error_message: Optional[str] = None
    retry_count: int = 0

@dataclass
class PageNode:
    """页面节点数据结构"""
    url: str
    parent_url: Optional[str]
    depth: int
    html: str
    text: str
    status_code: int
    content_type: str
    crawl_metadata: CrawlMetadata
    snapshot_path: Optional[str] = None  # 页面快照保存路径
    extracted_links: List[str] = field(default_factory=list)  # 从body提取的相关链接
    interactive_content: Dict[str, Any] = field(default_factory=dict)  # 交互内容（点击后显示）
    children: List["PageNode"] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典格式，便于JSON序列化"""
        return {
            'url': self.url,
            'parent_url': self.parent_url,
            'depth': self.depth,
            'html': self.html,
            'text': self.text,
            'status_code': self.status_code,
            'content_type': self.content_type,
            'crawl_metadata': {
                'crawl_run_id': self.crawl_metadata.crawl_run_id,
                'crawl_time': self.crawl_metadata.crawl_time,
                'crawler_version': self.crawl_metadata.crawler_version,
                'max_depth': self.crawl_metadata.max_depth,
                'success': self.crawl_metadata.success,
                'error_message': self.crawl_metadata.error_message,
                'retry_count': self.crawl_metadata.retry_count
            },
            'snapshot_path': self.snapshot_path,
            'extracted_links': self.extracted_links,
            'interactive_content': self.interactive_content,
            'children': [child.to_dict() for child in self.children]
        }
```

### 3.3 存储格式要求

**必须同时保存以下格式：**
1. **JSON格式**：完整的树形结构数据（`project_{project_id}_{crawl_run_id}.json`）
2. **页面快照**：每个页面的visual screenshot（`snapshots/{url_hash}.png`）
3. **元数据文件**：版本控制信息（`metadata_{crawl_run_id}.json`）

---

## 4. 爬虫实现要求

### 4.1 递归爬虫约束

```python
class CrawlConstraints:
    """爬虫约束配置"""
    max_depth: int = 3  # 最大递归深度
    
    # 排除的URL模式（header/footer/导航栏等）
    excluded_patterns = [
        r'/privacy.*',
        r'/terms.*', 
        r'/contact.*',
        r'/about.*',
        r'.*facebook.*',
        r'.*twitter.*',
        r'.*linkedin.*',
        r'.*instagram.*',
        r'/search.*',
        r'/login.*',
        r'/register.*',
        r'#.*',  # 锚点链接
        r'javascript:.*',
        r'mailto:.*',
        r'tel:.*'
    ]
    
    # 只爬取body中的相关链接
    body_selectors = [
        'main a[href]',
        'article a[href]', 
        '.content a[href]',
        '#main-content a[href]',
        '.program-details a[href]',
        '.admission-requirements a[href]'
    ]
```

### 4.2 全面信息抓取要求

**必须实现的功能：**

1. **基础内容抓取**：静态HTML文本和结构
2. **交互内容抓取**：
   - 点击按钮后显示的隐藏内容
   - Tab切换后的内容
   - 折叠/展开面板的内容
   - 下拉菜单的选项
3. **动态加载内容**：
   - AJAX加载的内容
   - 滚动加载的内容

```python
async def extract_interactive_content(page):
    """抓取交互式内容"""
    interactive_data = {}
    
    # 1. 点击所有可点击元素获取隐藏内容
    clickable_elements = await page.query_selector_all(
        'button, .expandable, .collapsible, [data-toggle], .tab'
    )
    
    for element in clickable_elements:
        try:
            await element.click()
            await page.wait_for_timeout(1000)  # 等待内容加载
            content = await page.content()
            interactive_data[f"clicked_{element}"] = content
        except:
            continue
    
    # 2. 处理Tab切换
    tabs = await page.query_selector_all('.tab, [role="tab"]')
    for tab in tabs:
        try:
            await tab.click()
            await page.wait_for_timeout(500)
            content = await page.content()
            interactive_data[f"tab_{tab}"] = content
        except:
            continue
    
    return interactive_data
```

### 4.3 页面快照功能

```python
async def save_page_snapshot(page, url, snapshot_dir):
    """保存页面快照"""
    try:
        # 创建快照目录
        os.makedirs(snapshot_dir, exist_ok=True)
        
        # 生成文件名（URL hash）
        url_hash = hashlib.md5(url.encode()).hexdigest()
        snapshot_path = f"{snapshot_dir}/{url_hash}.png"
        
        # 截取完整页面截图
        await page.screenshot(path=snapshot_path, full_page=True)
        
        return snapshot_path
    except Exception as e:
        print(f"快照保存失败 {url}: {e}")
        return None
```

---

## 5. 错误处理与重试机制

### 5.1 错误分类与标识

```python
class CrawlError:
    """爬虫错误分类"""
    NETWORK_ERROR = "network_error"
    TIMEOUT_ERROR = "timeout_error" 
    PERMISSION_ERROR = "permission_error"
    CONTENT_ERROR = "content_error"
    UNKNOWN_ERROR = "unknown_error"

def mark_failed_url(node: PageNode, error_type: str, error_msg: str):
    """标记失败的URL"""
    node.crawl_metadata.success = False
    node.crawl_metadata.error_message = f"{error_type}: {error_msg}"
    node.status_code = -1
    node.html = ""
    node.text = f"CRAWL_FAILED: {error_msg}"
```

### 5.2 失败URL重试功能

```python
def extract_failed_urls(json_data: Dict) -> List[str]:
    """从JSON数据中提取爬虫失败的URL"""
    failed_urls = []
    
    def traverse_node(node_data):
        if not node_data.get('crawl_metadata', {}).get('success', True):
            failed_urls.append(node_data['url'])
        
        for child in node_data.get('children', []):
            traverse_node(child)
    
    traverse_node(json_data)
    return failed_urls

async def retry_failed_urls(failed_urls: List[str], max_retries: int = 3):
    """重新爬取失败的URL"""
    retry_results = {}
    
    for url in failed_urls:
        for attempt in range(max_retries):
            try:
                # 重新爬取逻辑
                result = await crawl_single_page(url)
                if result.crawl_metadata.success:
                    retry_results[url] = result
                    break
            except Exception as e:
                if attempt == max_retries - 1:
                    retry_results[url] = create_failed_node(url, str(e))
    
    return retry_results
```

---

## 6. 数据库设计（PostgreSQL 推荐）

### 6.1 `project_pages` 表（邻接表）

```sql
CREATE TABLE project_pages (
    id            SERIAL PRIMARY KEY,
    project_id    UUID NOT NULL,
    url           TEXT NOT NULL,
    parent_url    TEXT,
    depth         INT  NOT NULL,
    html          BYTEA,
    text          TEXT,
    status_code   SMALLINT,
    content_type  TEXT,
    crawl_time    TIMESTAMPTZ DEFAULT now(),
    crawl_run_id  UUID,
    snapshot_path TEXT,
    success       BOOLEAN DEFAULT TRUE,
    error_message TEXT,
    retry_count   INT DEFAULT 0
);
CREATE INDEX idx_project_pages_project   ON project_pages(project_id);
CREATE INDEX idx_project_pages_parent    ON project_pages(parent_url);
CREATE INDEX idx_project_pages_failed    ON project_pages(success) WHERE success = FALSE;
```

### 6.2 `crawl_runs` 表（版本控制）

```sql
CREATE TABLE crawl_runs (
    id          UUID PRIMARY KEY,
    project_id  UUID NOT NULL,
    run_time    TIMESTAMPTZ DEFAULT now(),
    status      TEXT CHECK (status IN ('succeeded','failed','partial')),
    crawler_version TEXT,
    max_depth   INT,
    total_pages INT,
    failed_pages INT,
    config      JSONB  -- 爬虫配置参数
);
```

> **版本化策略**：`project_pages.crawl_run_id` 指向一次完整抓取；保留历史便于差分与回滚。

---

## 7. 字段抽取与填充

1. **正则/规则优先**：ETS Code、申请费等格式固定字段。
2. **LLM 兜底**：将项目所有 `text` 合并，Prompt "请生成 JSON：{ets\_code, fee, ...}"。
3. **置信度阈值**：低于 0.75 的字段进入人工审核队列。

---

## 8. 全文检索与向量索引

* **Postgres** `tsvector`：快速关键词定位。
* **Elastic / OpenSearch**：可加权域搜索，支持高亮。
* **向量库 (pgvector / FAISS)**：供 LLM RAG 检索片段。

---

## 9. 增量更新流程

1. 读取 `last-modified / ETag`，跳过未变页面。
2. 对比新旧哈希，生成差异报告。
3. 仅插入变更记录，旧版本保留供审计。

---

## 10. 并发与性能

| 组件  | 技术                    | 建议并发           |
| --- | --------------------- | -------------- |
| 爬虫  | Playwright + asyncio  | ≤200 请求/秒/机    |
| 抽取  | OpenAI GPT-4o (Batch) | 10k tokens/req |
| 数据库 | PostgreSQL            | 分区表/并行查询       |

---

## 11. 实施检查清单

### 11.1 爬虫功能验证
- [ ] JSON数据结构完整性测试
- [ ] 页面快照功能测试（确认可行性）
- [ ] 递归深度控制测试
- [ ] Header/Footer链接过滤测试
- [ ] 交互内容抓取测试
- [ ] 错误处理和重试机制测试

### 11.2 数据质量检查
- [ ] 失败URL标识准确性
- [ ] 元数据完整性
- [ ] 版本控制功能
- [ ] 快照与HTML内容一致性

---

## 12. Quick Start（最小可用集）

1. **内存树 → JSON**：抓 1 所学校，生成 `project.json`。
2. **快照测试**：验证页面截图功能可行性。
3. **写入 Postgres**：执行上面 DDL，导入脚本。
4. **字段抽取脚本**：跑正则 + LLM，输出 CSV。
5. **失败重试测试**：模拟网络失败，验证重试机制。
6. **人工审核**：用 Streamlit 对低置信度行打勾确认。

---

## 13. 附录

### 13.1 递归查询示例（Postgres）

```sql
WITH RECURSIVE page_tree AS (
  SELECT * FROM project_pages WHERE url = 'ROOT_URL'
  UNION ALL
  SELECT p.*
  FROM project_pages p
  JOIN page_tree pt ON p.parent_url = pt.url
)
SELECT * FROM page_tree;
```

### 13.2 Python 抓取片段

```python
async def crawl_page(page, parent_url, depth, constraints):
    # 抓取基础内容
    html = await page.content()
    text = BeautifulSoup(html, 'lxml').get_text("\n", strip=True)
    
    # 抓取交互内容
    interactive_content = await extract_interactive_content(page)
    
    # 保存快照
    snapshot_path = await save_page_snapshot(page, page.url, "snapshots")
    
    # 提取body中的相关链接
    extracted_links = await extract_body_links(page, constraints.body_selectors)
    
    # 创建元数据
    metadata = CrawlMetadata(
        crawl_run_id=generate_run_id(),
        crawl_time=datetime.utcnow().isoformat(),
        crawler_version="1.0.0",
        max_depth=constraints.max_depth,
        success=True
    )
    
    node = PageNode(
        url=page.url, 
        parent_url=parent_url, 
        depth=depth,
        html=html, 
        text=text, 
        status_code=page.status,
        content_type='text/html', 
        crawl_metadata=metadata,
        snapshot_path=snapshot_path,
        extracted_links=extracted_links,
        interactive_content=interactive_content
    )
    
    # 递归抓取子页面
    if depth < constraints.max_depth:
        for link in extracted_links:
            if not should_exclude_url(link, constraints.excluded_patterns):
                child_node = await crawl_page(await page.goto(link), page.url, depth + 1, constraints)
                node.children.append(child_node)
    
    return node
```

---

> **建议**：先用 3-5 个项目跑通完整Pipeline（包括快照功能测试），验证字段抽取准确率 ≥90% 且错误处理机制完善后再全量扩展。
