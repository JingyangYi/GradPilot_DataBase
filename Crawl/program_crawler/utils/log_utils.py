import re
import csv
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime

__all__ = [
    "print_zero_success_projects",
    "export_failed_urls_to_csv",
]


def print_zero_success_projects(log_path: str) -> List[Tuple[str, str]]:
    """Scan a crawl log and print (return) all projects whose success rate is 0%.

    Parameters
    ----------
    log_path : str
        Path to the log file generated by ``ProgramSpider`` (usually under
        ``crawl/log/``).

    Returns
    -------
    List[Tuple[str, str]]
        A list of tuples containing (project_name, root_url) for projects that had 
        *zero* successful pages.

    Notes
    -----
    1. The function looks for two patterns::

           项目名称: <项目名称>
           根URL: <根URL>
           ...
           项目完成: <项目名称>
           ...
           成功率: 0.0%

       It pairs each project start block with its completion block.
    2. Only projects whose success rate equals exactly ``0.0%`` are reported.
       If you need a different threshold, modify the condition in the code.
    """
    log_file = Path(log_path)
    if not log_file.is_file():
        raise FileNotFoundError(f"Log file not found: {log_path}")

    failed_projects: List[Tuple[str, str]] = []
    project_info: Dict[str, str] = {}  # project_name -> root_url
    current_project: str | None = None
    last_completed_project: str | None = None

    # 匹配项目开始时的信息
    project_start_pattern = re.compile(r"项目名称: (?P<name>.+)")
    root_url_pattern = re.compile(r"根URL: (?P<url>.+)")
    
    # 匹配项目完成时的信息
    project_complete_pattern = re.compile(r"项目完成: (?P<name>.+)")
    success_pattern = re.compile(r"成功率: (?P<rate>[\d.]+)%")

    with log_file.open("r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            # 检查项目开始
            start_match = project_start_pattern.search(line)
            if start_match:
                current_project = start_match.group("name").strip()
                continue
            
            # 检查根URL
            if current_project:
                url_match = root_url_pattern.search(line)
                if url_match:
                    root_url = url_match.group("url").strip()
                    project_info[current_project] = root_url
                    current_project = None  # 重置，等待下一个项目
                    continue
            
            # 检查项目完成
            complete_match = project_complete_pattern.search(line)
            if complete_match:
                last_completed_project = complete_match.group("name").strip()
                continue

            # 检查成功率
            if last_completed_project is not None:
                success_match = success_pattern.search(line)
                if success_match:
                    rate = float(success_match.group("rate"))
                    if rate == 0.0:
                        root_url = project_info.get(last_completed_project, "未知URL")
                        failed_projects.append((last_completed_project, root_url))
                    # 重置直到下一个项目完成
                    last_completed_project = None

    if failed_projects:
        print("以下项目抓取成功率为 0% (完全失败):")
        print("-" * 80)
        for name, url in failed_projects:
            print(f"项目名称: {name}")
            print(f"根URL: {url}")
            print("-" * 80)
    else:
        print("未发现成功率为 0% 的项目。")

    return failed_projects


# ---------------------------------------------------------------------------
#  New utility: export_failed_urls_to_csv
# ---------------------------------------------------------------------------

def export_failed_urls_to_csv(
    failed_projects: List[Tuple[str, str]],
    program_urls_path: str,
    output_path: str | None = None,
) -> int:
    """Export rows whose *root URL* appears in *failed_projects* to a new CSV.

    Parameters
    ----------
    failed_projects : List[Tuple[str, str]]
        Output of :pyfunc:`print_zero_success_projects`. Each tuple contains
        *(project_name, root_url)*.
    program_urls_path : str
        Path to the ``program_urls.csv`` file.
    output_path : str | None, optional
        Location of the new CSV. If *None*, the file will be created alongside
        *program_urls_path* with the name ``failed_urls.csv``.

    Returns
    -------
    int
        Number of rows written to *output_path*.
    """

    # Determine output path lazily.
    program_urls_file = Path(program_urls_path)
    if output_path is None:
        output_path = program_urls_file.with_name("failed_urls.csv")
    output_file = Path(output_path)

    # Build a *set* of failed *root URLs* for quick lookup.
    failed_url_set = {url.strip() for _name, url in failed_projects if url}
    if not failed_url_set:
        print("[export_failed_urls_to_csv] 没有需要导出的失败项目 (failed_url_set 为空)。")
        return 0

    written = 0
    with program_urls_file.open("r", encoding="utf-8", newline="") as fin, \
            output_file.open("w", encoding="utf-8", newline="") as fout:
        reader = csv.DictReader(fin)
        writer = csv.DictWriter(fout, fieldnames=reader.fieldnames)
        writer.writeheader()

        for row in reader:
            if row.get("program_url", "").strip() in failed_url_set:
                writer.writerow(row)
                written += 1

    print(
        f"[export_failed_urls_to_csv] 已在 {output_file} 中写入 {written} 行失败记录。"
    )
    return written


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Scan crawl log, print zero-success projects and export their URLs."
    )
    parser.add_argument(
        "--program-urls",
        default="/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/crawl/program_urls.csv",
        help="Path to program_urls.csv (default: crawl/program_urls.csv)",
    )

    time = datetime.now().strftime("%Y%m%d_%H%M%S")
    parser.add_argument(
        "--output",
        default=f"/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/crawl/failed_urls_{time}.csv",
        help="Path to the failed_urls.csv to generate (default: alongside program_urls.csv)",
    )

    args = parser.parse_args()

    log_path = "/Users/yijingyang/Library/CloudStorage/OneDrive-个人/GradPilot/ProgramDB/crawl/log/crawl_test_20250801_132600.log"
    failed = print_zero_success_projects(log_path)
    export_failed_urls_to_csv(failed, args.program_urls, args.output)