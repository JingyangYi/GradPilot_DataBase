import scrapy
import csv
import os
from datetime import datetime
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import re
from scrapy import signals
from scrapy.exceptions import DontCloseSpider
from ..url_filter import filter_url
from ..items import ProgramPageItem

# =============================================================================
# ProgramSpider â€” GradPilot å®šåˆ¶çˆ¬è™«
# -----------------------------------------------------------------------------
# åŠŸèƒ½æ¦‚è§ˆ
#   1. é€ä¸ªâ€œé¡¹ç›®(project)â€é¡ºåºçˆ¬å–ï¼šæ¯ä¸ªé¡¹ç›®ä»£è¡¨ä¸€æ¡ CSV è®°å½•ï¼ˆåŒ…å« UUIDã€é¡¹ç›®
#      åç§°ã€æ ¹ URL ç­‰ï¼‰ã€‚æ ¹é¡µé¢ + åŒåŸŸåä¸”é”šæ–‡æœ¬/URL åŒ…å«ç™½åå•å…³é”®è¯çš„è‹¥å¹²å­
#      é¡µé¢ï¼ˆæ·±åº¦â‰¤1ï¼‰ä¼šè¢«æŠ“å–ã€‚
#   2. æ‰‹åŠ¨è¯·æ±‚è®¡æ•°(request_counters) + spider_idle ä¿¡å·ï¼šé¿å…åŒæ—¶è°ƒåº¦æˆç™¾ä¸Šåƒä¸ª
#      è¯·æ±‚å¯¼è‡´ DEPTH_LIMIT æˆ–å¹¶å‘å‹åŠ›ï¼›å½“å‰é¡¹ç›®å…¨éƒ¨å®Œæˆåå†å¯åŠ¨ä¸‹ä¸€ä¸ªé¡¹ç›®ã€‚
#   3. å…¨å±€å»é‡ç­–ç•¥ï¼šä½¿ç”¨ per-project çš„ seen_urls é›†åˆè‡ªè¡Œå»é‡ï¼Œå¹¶ç»Ÿä¸€ç»™æ‰€æœ‰æ–°çš„
#      Request åŠ  `dont_filter=True`ï¼Œä»è€Œä¿è¯ â€œè®¡æ•°å™¨ == å®é™…æ’é˜Ÿè¯·æ±‚æ•°â€ã€‚
#   4. ç¨³å¥çš„é”™è¯¯å¤„ç†ï¼šerrback â†’ handle_error() ä¼šå³æ—¶é€’å‡è®¡æ•°å¹¶åœ¨è®¡æ•°å½’é›¶æ—¶ç›´æ¥
#      è°ƒç”¨ complete_project()ï¼Œä¿è¯æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥éƒ½èƒ½æ­£ç¡®æ”¶å°¾å¹¶è§£é”ä¸‹ä¸€ä¸ªé¡¹ç›®ã€‚
#
# ç»´æŠ¤è€…é¡»çŸ¥
#   â€¢ å¦‚æœè¦ä¿®æ”¹é“¾æ¥ç™½åå•ï¼Œè¯·æŸ¥çœ‹ crawl/program_crawler/url_filter.py
#   â€¢ è‹¥éœ€è°ƒèŠ‚å¹¶å‘/æ·±åº¦ç­‰çˆ¬è™«è®¾ç½®ï¼Œå¯åœ¨ crawl/run_crawler.py æˆ– settings.py ä¿®æ”¹ã€‚
#   â€¢ è®¡æ•°å™¨ / seen_urls / dont_filter ä¸‰è€…å¿…é¡»ä¿æŒä¸€è‡´ï¼Œå¦åˆ™çˆ¬è™«ä¼šå†æ¬¡å‡ºç°â€œå‰©ä½™è¯·æ±‚
#     != å®é™…è¯·æ±‚â€è€Œæå‰å…³é—­çš„é—®é¢˜ã€‚
# =============================================================================


class ProgramSpider(scrapy.Spider):
    name = 'program_spider'
    allowed_domains = []
    
    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        """è‡ªå®šä¹‰æ„é€ å‡½æ•°ï¼Œç”¨äºæ³¨å†Œ spider_idle ä¿¡å·å¤„ç†å™¨"""
        # ä»crawlerè®¾ç½®ä¸­è·å–URLsæ–‡ä»¶è·¯å¾„
        urls_file = crawler.settings.get('URLS_FILE')
        if urls_file:
            kwargs['csv_file'] = urls_file
            
        spider = super(ProgramSpider, cls).from_crawler(crawler, *args, **kwargs)
        crawler.signals.connect(spider.spider_idle, signal=signals.spider_idle)
        return spider
    
    def __init__(self, csv_file='program_urls.csv', start_index=0, *args, **kwargs):
        super(ProgramSpider, self).__init__(*args, **kwargs)
        
        self.csv_file = csv_file
        self.start_index = int(start_index)  # æ”¯æŒä»æŒ‡å®šç´¢å¼•å¼€å§‹
        self.project_queue = []
        self.current_project = None
        self.current_project_id = None
        self.request_counters = {}
        self.project_data = {}
        
        self.total_projects = 0
        self.completed_projects = 0
        self.failed_projects = 0
        self.is_processing_project = False
        
        self.load_projects()
        
    def load_projects(self):
        """ä»CSVæ–‡ä»¶åŠ è½½é¡¹ç›®åˆ—è¡¨"""
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™æ‹¼æ¥ç›¸å¯¹è·¯å¾„
        if os.path.isabs(str(self.csv_file)):
            csv_path = str(self.csv_file)
        else:
            # å‘ä¸Šä¸¤çº§ç›®å½•åˆ°crawlæ–‡ä»¶å¤¹ï¼Œç„¶ååŠ ä¸Šcsvæ–‡ä»¶å
            csv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), self.csv_file)
        
        self.logger.info(f"å°è¯•åŠ è½½CSVæ–‡ä»¶: {csv_path}")
        self.logger.info(f"æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {os.path.exists(csv_path)}")
        
        try:
            with open(csv_path, 'r', encoding='utf-8-sig') as f:  # å¤„ç†BOMå­—ç¬¦
                reader = csv.DictReader(f)
                all_projects = []
                for row in reader:
                    project = {
                        'id': row['id'],
                        'name': row['program_name'],
                        'url': row['program_url'],
                        'source_file': row['source_file']
                    }
                    all_projects.append(project)
                    
                    domain = urlparse(project['url']).netloc
                    if domain not in self.allowed_domains:
                        self.allowed_domains.append(domain)
                
                # ä»æŒ‡å®šç´¢å¼•å¼€å§‹åŠ è½½é¡¹ç›®
                original_total = len(all_projects)
                self.project_queue = all_projects[self.start_index:]
                self.total_projects = len(self.project_queue)
                self.completed_projects = self.start_index  # å·²è·³è¿‡çš„é¡¹ç›®ç®—ä½œå·²å®Œæˆ
                        
            self.logger.info("\n" + "="*80)
            self.logger.info(f"åŸå§‹æ€»é¡¹ç›®æ•°: {original_total}")
            if self.start_index > 0:
                self.logger.info(f"ä»ç´¢å¼• {self.start_index} å¼€å§‹ï¼Œè·³è¿‡äº† {self.start_index} ä¸ªé¡¹ç›®")
            self.logger.info(f"å°†è¦çˆ¬å– {self.total_projects} ä¸ªé¡¹ç›®")
            self.logger.info(f"å…è®¸çš„åŸŸå: {self.allowed_domains}")
            self.logger.info("å°†æŒ‰é¡ºåºé€ä¸ªé¡¹ç›®è¿›è¡Œçˆ¬å–")
            self.logger.info("="*80)
            
        except Exception as e:
            self.logger.error(f"åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {e}")
            # è¾“å‡ºæ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å¸®åŠ©è°ƒè¯•
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            # ä¸è¦ç»§ç»­è¿è¡Œï¼Œç¡®ä¿é—®é¢˜è¢«å‘ç°
            raise RuntimeError(f"CSVæ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­: {e}")
            
    def start_requests(self):
        """å¼€å§‹ç¬¬ä¸€ä¸ªé¡¹ç›®çš„çˆ¬å–ï¼Œå…¶ä»–é¡¹ç›®å°†åœ¨å‰ä¸€ä¸ªå®Œæˆåä¾æ¬¡å¯åŠ¨"""
        if self.project_queue:
            for request in self.start_next_project():
                yield request
        return
            
    def start_next_project(self):
        """å¯åŠ¨ä¸‹ä¸€ä¸ªé¡¹ç›®çš„çˆ¬å–"""
        if not self.project_queue:
            self.logger.info("\n" + "="*50)
            self.logger.info("æ‰€æœ‰é¡¹ç›®å·²å®Œæˆ")
            self.logger.info("="*50)
            return []
            
        if self.is_processing_project:
            self.logger.warning("å½“å‰æ­£åœ¨å¤„ç†é¡¹ç›®ï¼Œè·³è¿‡å¯åŠ¨æ–°é¡¹ç›®")
            return []
            
        self.current_project = self.project_queue.pop(0)
        project_id = self.current_project['id']
        self.current_project_id = project_id
        self.is_processing_project = True
        
        self.request_counters[project_id] = 0
        self.project_data[project_id] = {
            'project_id': project_id,
            'program_name': self.current_project['name'],
            'source_file': self.current_project['source_file'],
            'root_url': self.current_project['url'],
            'crawl_time': datetime.now().isoformat(),
            'pages': [],
            'total_pages': 0,
            'successful_pages': 0,
            'failed_pages': 0,
            'status': 'crawling',
            'seen_urls': set([self.current_project['url']])  # è®°å½•å·²è°ƒåº¦çš„ URLï¼Œé¿å…é‡å¤
        }
        
        # æ›´æ¸…æ™°çš„é¡¹ç›®å¼€å§‹æ—¥å¿—
        self.logger.info("\n" + "="*80)
        self.logger.info(f"å¼€å§‹çˆ¬å–é¡¹ç›® [{self.completed_projects + 1}/{self.total_projects}]")
        self.logger.info(f"é¡¹ç›®åç§°: {self.current_project['name']}")
        self.logger.info(f"é¡¹ç›®ID: {project_id}")
        self.logger.info(f"æ ¹URL: {self.current_project['url']}")
        self.logger.info(f"å‰©ä½™é¡¹ç›®æ•°: {len(self.project_queue)}")
        self.logger.info("="*80)
        
        old_count = self.request_counters[project_id]
        self.request_counters[project_id] += 1
        self.logger.info(f"[{project_id}] è®¡æ•°å™¨å˜æ›´: {old_count} -> {self.request_counters[project_id]}, æ“ä½œ: å¯åŠ¨æ ¹é¡µé¢è¯·æ±‚")
        
        # éªŒè¯URLæœ‰æ•ˆæ€§
        url = self.current_project['url']
        if not url or url.strip() in ['æš‚æ— ', 'N/A', 'None', ''] or not url.startswith(('http://', 'https://')):
            self.logger.warning(f"[{project_id}] è·³è¿‡æ— æ•ˆURL: {url}")
            # å…ˆå‡å°‘è®¡æ•°å™¨ï¼Œç„¶åå®Œæˆé¡¹ç›®
            self.request_counters[project_id] -= 1
            self._complete_project_sync(project_id)  # åŒæ­¥å®Œæˆé¡¹ç›®ï¼Œä¸yield Item
            # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªé¡¹ç›®
            if self.project_queue:
                yield from self.start_next_project()
            return
        
        request = scrapy.Request(
            url=url,
            callback=self.parse_page,
            errback=self.handle_error,
            meta={
                'project_id': project_id,
                'depth': 0,
                'is_root': True,
                'cookiejar': project_id,
            },
            dont_filter=True  # é¿å…å…¨å±€å»é‡å½±å“è®¡æ•°
        )
        # é‡ç½®æ ¹é¡µé¢çš„æ·±åº¦ä¸º0ï¼Œé¿å…æ·±åº¦é™åˆ¶é—®é¢˜
        request.meta['depth'] = 0
        yield request
        
    def parse_page(self, response):
        """è§£æé¡µé¢å†…å®¹"""
        project_id = response.meta['project_id']
        depth = response.meta.get('depth', 0)
        is_root = response.meta.get('is_root', False)
        
        # åœ¨è§£ææ¯ä¸ªé¡µé¢æ—¶è¾“å‡ºè¿›åº¦ä¿¡æ¯
        current_project_data = self.project_data[project_id]
        processed_pages = current_project_data['successful_pages'] + current_project_data['failed_pages']
        page_type = "æ ¹é¡µé¢" if is_root else f"å­é¡µé¢(æ·±åº¦{depth})"
        
        self.logger.info(f"[{project_id}] æ­£åœ¨å¤„ç†{page_type} (å·²å¤„ç†{processed_pages}ä¸ªé¡µé¢): {response.url[:100]}{'...' if len(response.url) > 100 else ''}")
        
        if not self.is_html_content(response):
            self.logger.info(f"[{project_id}] è·³è¿‡éHTMLå†…å®¹: {response.url}")
            
            # ä¸ºéHTMLå†…å®¹åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„é¡µé¢è®°å½•
            page_data = {
                'url': response.url,
                'depth': depth,
                'title': '',
                'content': f"éHTMLå†…å®¹ - Content-Type: {response.headers.get('Content-Type', b'').decode('utf-8')}",
                'links': [],
                'crawl_status': 'skipped_non_html'
            }
            self.project_data[project_id]['pages'].append(page_data)
            self.project_data[project_id]['failed_pages'] += 1
            
            # æ£€æŸ¥é¡¹ç›®æ˜¯å¦å®Œæˆ  
            old_count = self.request_counters[project_id]
            self.request_counters[project_id] -= 1
            self.logger.info(f"[{project_id}] è®¡æ•°å™¨å˜æ›´: {old_count} -> {self.request_counters[project_id]}, æ“ä½œ: å®ŒæˆéHTMLé¡µé¢å¤„ç†")
            self.logger.info(f"[{project_id}] å‰©ä½™è¯·æ±‚æ•°: {self.request_counters[project_id]}")
            
            if self.request_counters[project_id] <= 0:
                yield from self.complete_project(project_id)
            return
            
        try:
            # ğŸ¯ ä¸€æ¬¡è§£æHTMLï¼Œå¤šæ¬¡å¤ç”¨ - æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒ
            soup = BeautifulSoup(response.text, 'html.parser')
            
            page_data = {
                'url': response.url,
                'depth': depth,
                'title': self.extract_title_from_soup(soup),
                'content': self.extract_content_from_soup(soup),
                'links': [],
                'crawl_status': 'success'
            }
            
            self.project_data[project_id]['pages'].append(page_data)
            self.project_data[project_id]['successful_pages'] += 1
            
            # ä¿®æ”¹é“¾æ¥æå–æ¡ä»¶ï¼šå…è®¸æ ¹é¡µé¢(is_root=True)æˆ–æ·±åº¦å°äº1çš„é¡µé¢æå–é“¾æ¥
            is_root = response.meta.get('is_root', False)
            if depth < 1 or is_root:
                links = self.extract_links_from_soup(soup, response) # ä»…åŒ¹é…é”šæ–‡æœ¬
                page_data['links'] = links
                
                # è°ƒè¯•ä¿¡æ¯ï¼šå¦‚æœæ²¡æœ‰æå–åˆ°é“¾æ¥ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                if not links:
                    self.logger.warning(f"[{project_id}] é¡µé¢ {response.url} (depth={depth}, is_root={is_root}) æ²¡æœ‰æå–åˆ°ä»»ä½•ç¬¦åˆæ¡ä»¶çš„é“¾æ¥")
                else:
                    self.logger.info(f"[{project_id}] é¡µé¢ {response.url} (depth={depth}, is_root={is_root}) æå–åˆ° {len(links)} ä¸ªé“¾æ¥")
                
                # ä½¿ç”¨é¡¹ç›®çº§å…¨å±€é›†åˆè¿›è¡Œå»é‡ï¼Œä¿è¯è®¡æ•°å‡†ç¡®
                seen_urls_global = self.project_data[project_id].setdefault('seen_urls', set())
                new_requests = []
                num_duplication = 0
                # å¤„ç†æ–°çš„å­—å…¸æ ¼å¼links
                for link_info in links:
                    link_url = link_info["url"]
                    anchor_text = link_info["anchor_text"] 
                    matched_keyword = link_info["matched_keyword"]
                    
                    if link_url in seen_urls_global:
                        num_duplication += 1
                        continue
                    seen_urls_global.add(link_url)

                    if not filter_url(link_url, anchor_text=anchor_text): # ä»…åŒ¹é…é”šæ–‡æœ¬
                        self.logger.info(
                            f"[{project_id}] çˆ¬å–å­é“¾æ¥: {link_url} (é”šæ–‡æœ¬: '{anchor_text}', åŒ¹é…å…³é”®è¯: '{matched_keyword}')")

                        # ä¸ºæ ¹é¡µé¢çš„å­é“¾æ¥ä½¿ç”¨æ·±åº¦1ï¼Œé¿å…æ·±åº¦é™åˆ¶é—®é¢˜
                        child_depth = 1 if is_root else depth + 1
                        request = scrapy.Request(
                            url=link_url,
                            callback=self.parse_page,
                            errback=self.handle_error,
                            meta={
                                'project_id': project_id,
                                'depth': child_depth,
                                'is_root': False,
                                'cookiejar': project_id,
                            },
                            dont_filter=True  # é¿å… Scrapy å»é‡å¯¼è‡´è®¡æ•°å™¨å¤±é…
                        )
                        new_requests.append(request)

                # è®°å½•å»é‡åçš„æ–°è¯·æ±‚æ•°
                self.logger.info(f"[{project_id}] æå–é“¾æ¥å®Œæˆï¼Œå»é‡å‰ {len(links)} ä¸ªé“¾æ¥ï¼Œå»é‡å {len(new_requests)} ä¸ªæ–°è¯·æ±‚ï¼ˆå»é‡äº† {num_duplication} ä¸ªé‡å¤é“¾æ¥ï¼‰")
                
                # ä¸€æ¬¡æ€§æ›´æ–°è®¡æ•°å™¨ï¼ˆä»…ç»Ÿè®¡çœŸæ­£ä¼šè¢«è°ƒåº¦çš„è¯·æ±‚ï¼‰
                if new_requests:
                    old_count = self.request_counters[project_id]
                    self.request_counters[project_id] += len(new_requests)
                    self.logger.info(f"[{project_id}] è®¡æ•°å™¨å˜æ›´: {old_count} -> {self.request_counters[project_id]}, æ“ä½œ: æ·»åŠ {len(new_requests)}ä¸ªå­é¡µé¢è¯·æ±‚")
                    self.logger.info(
                        f"[{project_id}] æ·»åŠ  {len(new_requests)} ä¸ªæ–°è¯·æ±‚ï¼Œå½“å‰å‰©ä½™: {self.request_counters[project_id] - 1}")

                
                # å‘å‡ºæ‰€æœ‰è¯·æ±‚
                for request in new_requests:
                    yield request
                        
        except Exception as e:
            self.logger.error(f"[{project_id}] è§£æé¡µé¢å¤±è´¥ {response.url}: {e}")
            self.project_data[project_id]['failed_pages'] += 1
            
        # å…ˆæ£€æŸ¥å¹¶å¤„ç†å¾…å®Œæˆçš„é¡¹ç›®ï¼ˆæ¥è‡ªé”™è¯¯å¤„ç†ï¼‰
        if hasattr(self, '_projects_to_complete'):
            for pending_project_id in list(self._projects_to_complete):
                if self.request_counters.get(pending_project_id, 0) <= 0:
                    self._projects_to_complete.remove(pending_project_id)
                    yield from self.complete_project(pending_project_id)
        
        # æœ€åæ£€æŸ¥å½“å‰é¡¹ç›®æ˜¯å¦å®Œæˆï¼ˆåœ¨æ‰€æœ‰yieldæ“ä½œå®Œæˆåï¼‰
        old_count = self.request_counters[project_id]
        self.request_counters[project_id] -= 1
        self.logger.info(f"[{project_id}] è®¡æ•°å™¨å˜æ›´: {old_count} -> {self.request_counters[project_id]}, æ“ä½œ: å®Œæˆé¡µé¢è§£æ")
        processed_pages_after = current_project_data['successful_pages'] + current_project_data['failed_pages']
        self.logger.debug(f"[{project_id}] å·²å¤„ç†é¡µé¢: {processed_pages_after}, å‰©ä½™è¯·æ±‚æ•°: {self.request_counters[project_id]}")
        
        if self.request_counters[project_id] <= 0:
            yield from self.complete_project(project_id)
        
    def handle_error(self, failure):
        """å¤„ç†è¯·æ±‚é”™è¯¯ï¼ˆä½œä¸ºç”Ÿæˆå™¨ï¼Œå¯ yield item/requestï¼‰"""
        project_id = failure.request.meta.get('project_id')
        if not project_id:
            return

        # å¢å¼ºé”™è¯¯ä¿¡æ¯æ˜¾ç¤º
        error_info = []
        error_info.append(f"URL: {failure.request.url}")
        error_info.append(f"é”™è¯¯ç±»å‹: {failure.type.__name__}")
        error_info.append(f"é”™è¯¯æè¿°: {failure.value}")
        
        # å¦‚æœæ˜¯HTTPé”™è¯¯ï¼Œæ˜¾ç¤ºçŠ¶æ€ç å’Œå“åº”å¤´
        if hasattr(failure.value, 'response') and failure.value.response is not None:
            response = failure.value.response
            error_info.append(f"HTTPçŠ¶æ€ç : {response.status}")
            
            # æ˜¾ç¤ºå…³é”®å“åº”å¤´
            important_headers = ['server', 'content-type', 'set-cookie', 'cf-ray', 'user-agent']
            response_headers = []
            for header in important_headers:
                if header.encode() in response.headers:
                    value = response.headers.get(header).decode('utf-8', errors='ignore')[:200]
                    response_headers.append(f"{header}: {value}")
            
            if response_headers:
                error_info.append(f"å“åº”å¤´: {'; '.join(response_headers)}")
                
            # æ˜¾ç¤ºå“åº”ä½“å‰200å­—ç¬¦ï¼ˆå¦‚æœæœ‰ï¼‰
            if hasattr(response, 'text') and len(response.text) > 0:
                response_preview = response.text[:200].replace('\n', ' ').replace('\r', ' ')
                error_info.append(f"å“åº”é¢„è§ˆ: {response_preview}...")
        
        # æ˜¾ç¤ºè¯·æ±‚å¤´ä¿¡æ¯
        request_headers = []
        important_req_headers = ['user-agent', 'accept', 'accept-language', 'accept-encoding']
        for header in important_req_headers:
            if header.encode() in failure.request.headers:
                value = failure.request.headers.get(header).decode('utf-8', errors='ignore')[:100]
                request_headers.append(f"{header}: {value}")
        
        if request_headers:
            error_info.append(f"è¯·æ±‚å¤´: {'; '.join(request_headers)}")
        
        # è¾“å‡ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
        self.logger.error(f"[{project_id}] è¯·æ±‚å¤±è´¥è¯¦æƒ…:")
        for info in error_info:
            self.logger.error(f"[{project_id}]   {info}")
            
        self.project_data[project_id]['failed_pages'] += 1

        # æ›´æ–°è®¡æ•°å™¨
        old_count = self.request_counters[project_id]
        self.request_counters[project_id] -= 1
        self.logger.info(f"[{project_id}] è®¡æ•°å™¨å˜æ›´: {old_count} -> {self.request_counters[project_id]}, æ“ä½œ: å¤„ç†è¯·æ±‚é”™è¯¯")
        current_project_data = self.project_data[project_id]
        processed_pages = current_project_data['successful_pages'] + current_project_data['failed_pages']
        self.logger.info(f"[{project_id}] å·²å¤„ç†é¡µé¢: {processed_pages}, å‰©ä½™è¯·æ±‚æ•°: {self.request_counters[project_id]}")

        # è®°å½•å¤±è´¥åˆ°çŠ¶æ€æ–‡ä»¶
        self.record_failed_request(project_id, failure)

        # å¦‚æœå½“å‰é¡¹ç›®æ‰€æœ‰è¯·æ±‚éƒ½å·²å›æ”¶ï¼Œç«‹å³å®Œæˆé¡¹ç›®
        if self.request_counters[project_id] <= 0:
            self.logger.info(f"[{project_id}] é¡¹ç›®åœ¨é”™è¯¯å¤„ç†ä¸­å®Œæˆï¼Œç«‹å³æ”¶å°¾ â€¦")
            yield from self.complete_project(project_id)
    
    def record_failed_request(self, project_id, failure):
        """è®°å½•å¤±è´¥çš„è¯·æ±‚åˆ°å­¦ç§‘ä¸“é—¨çš„å¤±è´¥æ—¥å¿—æ–‡ä»¶"""
        try:
            import json
            import os
            from datetime import datetime
            
            # è·å–é¡¹ç›®ä¿¡æ¯
            project_data = self.project_data.get(project_id, {})
            program_name = project_data.get('program_name', 'Unknown')
            source_file_raw = project_data.get('source_file', 'unknown.csv')
            
            # ç»Ÿä¸€å¤„ç†source_fileï¼Œç§»é™¤æ‰€æœ‰å¯èƒ½çš„æ‰©å±•å
            source_file = source_file_raw.replace('.csv', '').replace('.json', '')
            
            # æå–å­¦ç§‘åç§°ï¼ˆå»æ‰å¯èƒ½çš„æ•°å­—åç¼€ï¼Œå¦‚"è®¡ç®—æœº_1" -> "è®¡ç®—æœº"ï¼‰
            subject_name = source_file.split('_')[0] if '_' in source_file else source_file
            
            # æ„å»ºå¤±è´¥æ—¥å¿—æ–‡ä»¶è·¯å¾„
            # __file__ = .../crawl/program_crawler/spiders/program_spider.py
            # éœ€è¦4ä¸ªdirnameåˆ°è¾¾crawlç›®å½•ï¼šspiders -> program_crawler -> crawl
            crawl_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            log_dir = os.path.join(crawl_dir, 'log', subject_name)
            
            # ç¡®ä¿å­¦ç§‘æ—¥å¿—ç›®å½•å­˜åœ¨
            if not os.path.exists(log_dir):
                os.makedirs(log_dir)
            
            failed_log_file = os.path.join(log_dir, f'failed_urls_{source_file}.json')
            
            # æ„å»ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
            error_msg = f"{failure.type.__name__}: {failure.value}"
            http_status = None
            response_headers = {}
            response_preview = ""
            
            if hasattr(failure.value, 'response') and failure.value.response is not None:
                response = failure.value.response
                http_status = response.status
                error_msg += f" (HTTP {response.status})"
                
                # æå–å“åº”å¤´
                important_headers = ['server', 'content-type', 'set-cookie', 'cf-ray']
                for header in important_headers:
                    if header.encode() in response.headers:
                        value = response.headers.get(header).decode('utf-8', errors='ignore')[:200]
                        response_headers[header] = value
                
                # æå–å“åº”ä½“é¢„è§ˆ
                if hasattr(response, 'text') and len(response.text) > 0:
                    response_preview = response.text[:200].replace('\n', ' ').replace('\r', ' ')
            
            # åˆ›å»ºå¤±è´¥è®°å½•ï¼ˆå»æ‰timestampå­—æ®µï¼‰
            failed_record = {
                'project_id': project_id,
                'program_name': program_name,
                'source_file': source_file,
                'url': failure.request.url,
                'error': error_msg,
                'error_type': failure.type.__name__,
                'http_status': http_status,
                'response_headers': response_headers,
                'response_preview': response_preview
            }
            
            # è¯»å–ç°æœ‰å¤±è´¥è®°å½•
            failed_records = []
            if os.path.exists(failed_log_file):
                with open(failed_log_file, 'r', encoding='utf-8') as f:
                    failed_records = json.load(f)
            
            # ğŸ¯ URLå»é‡ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒURLçš„å¤±è´¥è®°å½•
            existing_urls = {record['url'] for record in failed_records}
            if failure.request.url not in existing_urls:
                failed_records.append(failed_record)
                self.logger.info(f"[{project_id}] æ–°å¢å¤±è´¥URLè®°å½•: {failure.request.url}")
            else:
                self.logger.debug(f"[{project_id}] å¤±è´¥URLå·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤è®°å½•: {failure.request.url}")
            
            # ä¿å­˜å¤±è´¥è®°å½•ï¼ˆä»…åœ¨æœ‰æ–°è®°å½•æ—¶å†™å…¥ï¼‰
            if failure.request.url not in existing_urls:
                with open(failed_log_file, 'w', encoding='utf-8') as f:
                    json.dump(failed_records, f, ensure_ascii=False, indent=2)
                self.logger.info(f"[{project_id}] å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {failed_log_file}")
            else:
                self.logger.debug(f"[{project_id}] æœªä¿å­˜é‡å¤å¤±è´¥è®°å½•")
            
        except Exception as e:
            # çŠ¶æ€è®°å½•å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
            self.logger.warning(f"[{project_id}] è®°å½•å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {e}")
            
    def _complete_project_sync(self, project_id):
        """åŒæ­¥å®Œæˆé¡¹ç›®ï¼Œç›´æ¥å¤„ç†Itemè€Œä¸yieldï¼ˆç”¨äºRequestç”Ÿæˆå™¨ä¸­ï¼‰"""
        import time
        time.sleep(1)  # ç»™å¹¶å‘è¯·æ±‚1ç§’ç¼“å†²æ—¶é—´ï¼Œç¡®ä¿æ‰€æœ‰è¯·æ±‚éƒ½å·²å¤„ç†å®Œæ¯•
        
        self.logger.info(f"[{project_id}] æ­£åœ¨å®Œæˆé¡¹ç›®")
        
        # æ£€æŸ¥é¡¹ç›®æ˜¯å¦å·²ç»å®Œæˆè¿‡ï¼Œé¿å…é‡å¤å¤„ç†
        if not hasattr(self, '_completed_projects'):
            self._completed_projects = set()
        
        if project_id in self._completed_projects:
            self.logger.info(f"[{project_id}] é¡¹ç›®å·²ç»å®Œæˆè¿‡ï¼Œè·³è¿‡")
            return
            
        self._completed_projects.add(project_id)
        
        project_data = self.project_data[project_id]
        project_data['status'] = 'completed'
        project_data['total_pages'] = len(project_data['pages'])
        
        total_attempts = project_data['successful_pages'] + project_data['failed_pages']
        success_rate = (project_data['successful_pages'] / max(1, total_attempts)) * 100
        
        # æ›´æ¸…æ™°çš„é¡¹ç›®å®Œæˆæ—¥å¿—
        self.logger.info("\n" + "-"*60)
        self.logger.info(f"[{project_id}] é¡¹ç›®å®Œæˆ: {project_data['program_name']}")
        self.logger.info(f"[{project_id}]   - æ€»é¡µæ•°: {project_data['total_pages']}")
        self.logger.info(f"[{project_id}]   - æˆåŠŸé¡µæ•°: {project_data['successful_pages']}")
        self.logger.info(f"[{project_id}]   - å¤±è´¥é¡µæ•°: {project_data['failed_pages']}")
        self.logger.info(f"[{project_id}]   - æˆåŠŸç‡: {success_rate:.1f}%")
        self.logger.info("-"*60)
        
        # ç›´æ¥é€šè¿‡pipelineå¤„ç†itemï¼Œä¸é€šè¿‡yield
        item = ProgramPageItem()
        item['project_id'] = project_data['project_id']
        item['program_name'] = project_data['program_name']
        item['source_file'] = project_data['source_file']
        item['root_url'] = project_data['root_url']
        item['crawl_time'] = project_data['crawl_time']
        item['pages'] = project_data['pages']
        item['total_pages'] = project_data['total_pages']
        item['status'] = project_data['status']
        
        # ç›´æ¥è°ƒç”¨pipelineå¤„ç†item
        self.crawler.engine.scraper.itemproc.process_item(item, self)
        
        self.completed_projects += 1
        self.is_processing_project = False  # é‡Šæ”¾å½“å‰é¡¹ç›®çŠ¶æ€
        
        # ä¸åœ¨æ­¤å¤„ç›´æ¥å¯åŠ¨ä¸‹ä¸€ä¸ªé¡¹ç›®ï¼Œè€Œæ˜¯ç•™ç»™ spider_idle ä¿¡å·ç»Ÿä¸€è°ƒåº¦ï¼Œ
        # ä»¥é¿å…æ·±åº¦å åŠ å¯¼è‡´çš„ DEPTH_LIMIT ä¸¢åŒ…
        if self.project_queue:
            self.logger.info(f"[{project_id}] ä»æœ‰ {len(self.project_queue)} ä¸ªé¡¹ç›®å¾…çˆ¬ï¼Œå°†åœ¨çˆ¬è™«ç©ºé—²æ—¶ç»§ç»­ã€‚")
        else:
            self.logger.info("\n" + "="*50)
            self.logger.info("æ‰€æœ‰é¡¹ç›®å·²å®Œæˆ")
            self.logger.info(f"å®Œæˆç‡: {self.completed_projects}/{self.total_projects} (100%)")
            self.logger.info("="*50)
    
    def complete_project(self, project_id):
        """å®Œæˆå½“å‰é¡¹ç›®ï¼Œè¾“å‡ºç»Ÿè®¡ä¿¡æ¯å¹¶å¼€å§‹ä¸‹ä¸€ä¸ªé¡¹ç›®ï¼ˆç”Ÿæˆå™¨ç‰ˆæœ¬ï¼Œç”¨äºæ­£å¸¸æµç¨‹ï¼‰"""
        import time
        time.sleep(1)  # ç»™å¹¶å‘è¯·æ±‚1ç§’ç¼“å†²æ—¶é—´ï¼Œç¡®ä¿æ‰€æœ‰è¯·æ±‚éƒ½å·²å¤„ç†å®Œæ¯•
        
        self.logger.info(f"[{project_id}] æ­£åœ¨å®Œæˆé¡¹ç›®")
        
        # æ£€æŸ¥é¡¹ç›®æ˜¯å¦å·²ç»å®Œæˆè¿‡ï¼Œé¿å…é‡å¤å¤„ç†
        if not hasattr(self, '_completed_projects'):
            self._completed_projects = set()
        
        if project_id in self._completed_projects:
            self.logger.info(f"[{project_id}] é¡¹ç›®å·²ç»å®Œæˆè¿‡ï¼Œè·³è¿‡")
            return
            
        self._completed_projects.add(project_id)
        
        project_data = self.project_data[project_id]
        project_data['status'] = 'completed'
        project_data['total_pages'] = len(project_data['pages'])
        
        total_attempts = project_data['successful_pages'] + project_data['failed_pages']
        success_rate = (project_data['successful_pages'] / max(1, total_attempts)) * 100
        
        # æ›´æ¸…æ™°çš„é¡¹ç›®å®Œæˆæ—¥å¿—
        self.logger.info("\n" + "-"*60)
        self.logger.info(f"[{project_id}] é¡¹ç›®å®Œæˆ: {project_data['program_name']}")
        self.logger.info(f"[{project_id}]   - æ€»é¡µæ•°: {project_data['total_pages']}")
        self.logger.info(f"[{project_id}]   - æˆåŠŸé¡µæ•°: {project_data['successful_pages']}")
        self.logger.info(f"[{project_id}]   - å¤±è´¥é¡µæ•°: {project_data['failed_pages']}")
        self.logger.info(f"[{project_id}]   - æˆåŠŸç‡: {success_rate:.1f}%")
        self.logger.info("-"*60)
        
        item = ProgramPageItem()
        item['project_id'] = project_data['project_id']
        item['program_name'] = project_data['program_name']
        item['source_file'] = project_data['source_file']
        item['root_url'] = project_data['root_url']
        item['crawl_time'] = project_data['crawl_time']
        item['pages'] = project_data['pages']
        item['total_pages'] = project_data['total_pages']
        item['status'] = project_data['status']
        
        yield item
        
        self.completed_projects += 1
        self.is_processing_project = False  # é‡Šæ”¾å½“å‰é¡¹ç›®çŠ¶æ€
        
        # ä¸åœ¨æ­¤å¤„ç›´æ¥å¯åŠ¨ä¸‹ä¸€ä¸ªé¡¹ç›®ï¼Œè€Œæ˜¯ç•™ç»™ spider_idle ä¿¡å·ç»Ÿä¸€è°ƒåº¦ï¼Œ
        # ä»¥é¿å…æ·±åº¦å åŠ å¯¼è‡´çš„ DEPTH_LIMIT ä¸¢åŒ…
        if self.project_queue:
            self.logger.info(f"[{project_id}] ä»æœ‰ {len(self.project_queue)} ä¸ªé¡¹ç›®å¾…çˆ¬ï¼Œå°†åœ¨çˆ¬è™«ç©ºé—²æ—¶ç»§ç»­ã€‚")
        else:
            self.logger.info("\n" + "="*50)
            self.logger.info("æ‰€æœ‰é¡¹ç›®å·²å®Œæˆ")
            self.logger.info(f"å®Œæˆç‡: {self.completed_projects}/{self.total_projects} (100%)")
            self.logger.info("="*50)
        
    def is_html_content(self, response):
        """æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºHTMLå†…å®¹"""
        content_type = response.headers.get('Content-Type', b'').decode('utf-8').lower()
        return 'text/html' in content_type
        
    def extract_title(self, response):
        """æå–é¡µé¢æ ‡é¢˜ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼Œå»ºè®®ä½¿ç”¨extract_title_from_soupï¼‰"""
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            return self.extract_title_from_soup(soup)
        except:
            return ""
            
    def extract_title_from_soup(self, soup):
        """ä»soupå¯¹è±¡æå–é¡µé¢æ ‡é¢˜"""
        try:
            if soup is None:
                return ""
            title_tag = soup.find('title')
            return title_tag.get_text().strip() if title_tag else ""
        except:
            return ""
            
    def extract_content(self, response):
        """æå–é¡µé¢å†…å®¹ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼Œå»ºè®®ä½¿ç”¨extract_content_from_soupï¼‰"""
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            return self.extract_content_from_soup(soup)
        except Exception as e:
            self.logger.error(f"å†…å®¹æå–å¤±è´¥: {e}")
            return ""
            
    def extract_content_from_soup(self, soup):
        """ä»soupå¯¹è±¡æå–é¡µé¢å†…å®¹ï¼ˆå¸¦æ–‡æœ¬å»é‡ï¼‰"""
        try:
            if soup is None:
                return ""
                
            for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
                script.decompose()
                
            content_parts = []
            
            content_parts.extend(self.extract_text_elements(soup))
            content_parts.extend(self.extract_tables(soup))
            
            # åº”ç”¨æ–‡æœ¬å»é‡ï¼šè¿‡æ»¤æ‰é‡å¤çš„å¤§æ®µæ–‡æœ¬ï¼Œé¿å…å“åº”å¼è®¾è®¡ã€SEOé‡å¤ç­‰å¯¼è‡´çš„å†—ä½™
            deduplicated_parts = self.deduplicate_text_content(content_parts)
            
            return '\n'.join(filter(None, deduplicated_parts))
            
        except Exception as e:
            self.logger.error(f"å†…å®¹æå–å¤±è´¥: {e}")
            return ""
    
    def deduplicate_text_content(self, content_parts):
        """
        æ–‡æœ¬å»é‡ï¼šè¡Œçº§å»é‡ + ç»Ÿä¸€è§„èŒƒåŒ– (æ”¹è¿›ç‰ˆ)

        æ”¹è¿›ç‚¹ï¼š
        1. å…ˆæŒ‰æ¢è¡Œæ‹†åˆ†ä¸ºè¡Œçº§ç‰‡æ®µï¼Œç²’åº¦æ›´ç»†ï¼›
        2. ç»Ÿä¸€å»æ‰å‰ç¼€æ ‡è®°ï¼ˆå¦‚ [HTML_TABLE] ç­‰ï¼‰å’Œè¡Œé¦–ç¬¦å·ï¼ˆâ€¢ã€-ã€æ•°å­—åºå·ç­‰ï¼‰ï¼›
        3. é•¿åº¦ <10 çš„è¡Œè§†ä¸ºå™ªå£°ç›´æ¥è¿‡æ»¤ï¼›
        4. ä½¿ç”¨è§„èŒƒåŒ–æ–‡æœ¬ï¼ˆå»ç©ºç™½ã€è½¬å°å†™ï¼‰åšå»é‡åˆ¤æ–­ã€‚
        """
        if not content_parts:
            return content_parts

        deduplicated_parts = []
        seen = set()

        for part in content_parts:
            if not part or not isinstance(part, str):
                continue

            # æ‹†åˆ†è¡Œï¼Œé¿å…æ•´å—æ–‡æœ¬å»é‡å¤±è´¥
            for line in part.split('\n'):
                text = line.strip()
                if not text:
                    continue

                # å»æ‰åƒ [HTML_TABLE]ã€[GRID_TABLE] ç­‰å‰ç¼€æ ‡è®°
                text_no_prefix = re.sub(r'^\[[A-Z_]+\]\s*', '', text)
                # å»æ‰è¡Œé¦– bullet / ç ´æŠ˜å· / åºå·ç­‰
                text_no_prefix = re.sub(r'^[â€¢â—\-\u2022\s\d\.\)\(]+', '', text_no_prefix)

                # è§„èŒƒåŒ–ï¼šå»å¤šä½™ç©ºç™½ã€è½¬å°å†™
                normalized = ' '.join(text_no_prefix.lower().split())
                if len(normalized) < 10:
                    # ä¿¡æ¯é‡è¿‡ä½ï¼Œè·³è¿‡
                    continue

                if normalized in seen:
                    continue
                seen.add(normalized)

                # ä¿ç•™åŸå§‹è¡Œæ–‡æœ¬ï¼ˆå«ç¬¦å·ï¼Œä¿æŒå¯è¯»æ€§ï¼‰
                deduplicated_parts.append(text)

        return deduplicated_parts
            
    def extract_text_elements(self, soup):
        """æå–å¸¸è§„æ–‡æœ¬å…ƒç´ """
        content_parts = []
        
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = tag.get_text().strip()
            if text:
                content_parts.append(f"[HEADING] {text}")
                
        for tag in soup.find_all('p'):
            text = tag.get_text().strip()
            if text and len(text) > 10:
                content_parts.append(text)
                
        for tag in soup.find_all(['ul', 'ol']):
            for li in tag.find_all('li'):
                item_text = li.get_text().strip()
                if item_text:
                    content_parts.append(f"â€¢ {item_text}")
                
        return content_parts
        
    def extract_tables(self, soup):
        """æå–å„ç§è¡¨æ ¼å½¢å¼çš„å†…å®¹"""
        content_parts = []
        
        for table in soup.find_all('table'):
            table_content = self.extract_html_table(table)
            if table_content:
                content_parts.append(f"[HTML_TABLE]\n{table_content}")
                
        for grid in soup.find_all(class_=re.compile(r'grid|row|col')):
            grid_content = self.extract_grid_layout(grid)
            if grid_content:
                content_parts.append(f"[GRID_TABLE]\n{grid_content}")
                
        for dl in soup.find_all('dl'):
            dl_content = self.extract_definition_list(dl)
            if dl_content:
                content_parts.append(f"[DEFINITION_LIST]\n{dl_content}")
                
        for card in soup.find_all(class_=re.compile(r'card|panel|box')):
            card_content = self.extract_card_content(card)
            if card_content:
                content_parts.append(f"[CARD]\n{card_content}")
                
        return content_parts
        
    def extract_html_table(self, table):
        """æå–HTMLè¡¨æ ¼å†…å®¹"""
        rows = []
        for tr in table.find_all('tr'):
            cells = []
            for cell in tr.find_all(['td', 'th']):
                cell_text = cell.get_text().strip()
                cells.append(cell_text)
            if cells:
                rows.append(' | '.join(cells))
        return '\n'.join(rows)
        
    def extract_grid_layout(self, grid):
        """æå–CSS Grid/Flexboxå¸ƒå±€å†…å®¹"""
        items = []
        for item in grid.find_all(class_=re.compile(r'col|item|cell')):
            text = item.get_text().strip()
            if text and len(text) > 5:
                items.append(text)
        return '\n'.join(items) if len(items) > 1 else ""
        
    def extract_definition_list(self, dl):
        """æå–å®šä¹‰åˆ—è¡¨å†…å®¹"""
        items = []
        current_term = ""
        
        for child in dl.children:
            if hasattr(child, 'name'):
                if child.name == 'dt':
                    current_term = child.get_text().strip()
                elif child.name == 'dd' and current_term:
                    definition = child.get_text().strip()
                    items.append(f"{current_term}: {definition}")
                    current_term = ""
                    
        return '\n'.join(items)
        
    def extract_card_content(self, card):
        """æå–å¡ç‰‡å¼å¸ƒå±€å†…å®¹"""
        title = ""
        content = ""
        
        title_elem = card.find(class_=re.compile(r'title|header|heading'))
        if title_elem:
            title = title_elem.get_text().strip()
            
        content_elem = card.find(class_=re.compile(r'content|body|text'))
        if content_elem:
            content = content_elem.get_text().strip()
        else:
            content = card.get_text().strip()
            
        if title and content:
            return f"{title}\n{content}"
        return content if content else ""
        
    def extract_links(self, response):
        """æå–é¡µé¢é“¾æ¥å¹¶è¿›è¡Œè¿‡æ»¤ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼Œå»ºè®®ä½¿ç”¨extract_links_from_soupï¼‰"""
        try:
            soup = BeautifulSoup(response.text, 'html.parser')
            return self.extract_links_from_soup(soup, response)
        except Exception as e:
            self.logger.error(f"é“¾æ¥æå–å¤±è´¥: {e}")
            return []
            
    def extract_links_from_soup(self, soup, response):
        """ä»soupå¯¹è±¡æå–é¡µé¢é“¾æ¥å¹¶è¿›è¡Œè¿‡æ»¤ï¼›ä»…è·å–é”šæ–‡æœ¬åŒ¹é…çš„links"""
        try:
            if soup is None:
                return []
                
            # ä» response ä¸­è·å– project_id
            project_id = response.meta.get('project_id', 'unknown')
            
            # ç»Ÿè®¡æ‰€æœ‰é“¾æ¥
            all_links = soup.find_all('a', href=True)
            self.logger.info(f"[{project_id}] é¡µé¢æ€»é“¾æ¥æ•°: {len(all_links)}")
            
            # # ä¸´æ—¶è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰åŸå§‹é“¾æ¥çš„é”šæ–‡æœ¬
            # self.logger.info(f"[{project_id}] === åŸå§‹é“¾æ¥é”šæ–‡æœ¬åˆ—è¡¨ ===")
            # for i, link in enumerate(all_links):  # åªæ˜¾ç¤ºå‰20ä¸ªé¿å…æ—¥å¿—è¿‡é•¿
            #     anchor_text = link.get_text().strip()
            #     href = link.get('href', 'N/A')
            #     self.logger.info(f"[{project_id}] åŸå§‹é“¾æ¥{i+1}: '{anchor_text}' -> {href}")
            
            # åˆ é™¤å¯¼èˆªå…ƒç´  - å‡å°‘è¯¯åˆ ï¼Œåªåˆ é™¤æ˜ç¡®çš„å¯¼èˆªå’Œé¡µè„š
            nav_selectors = ['footer', '.menu', "header"]  # ç§»é™¤äº†'nav'é¿å…è¯¯åˆ é¡µé¢ä¸»è¦å†…å®¹
            removed_nav_count = 0
            for selector in nav_selectors:
                nav_elements = soup.select(selector)
                for nav_elem in nav_elements:
                    nav_elem.decompose()
                    removed_nav_count += len(nav_elem.find_all('a', href=True))
            
            remaining_links = soup.find_all('a', href=True)
            self.logger.info(f"[{project_id}] ç§»é™¤å¯¼èˆªåé“¾æ¥æ•°: {len(remaining_links)} (ç§»é™¤äº† {removed_nav_count} ä¸ªå¯¼èˆªé“¾æ¥)")
            
            # # ä¸´æ—¶è°ƒè¯•ï¼šæ‰“å°ç§»é™¤å¯¼èˆªåçš„é“¾æ¥é”šæ–‡æœ¬
            # self.logger.info(f"[{project_id}] === ç§»é™¤å¯¼èˆªåé“¾æ¥é”šæ–‡æœ¬åˆ—è¡¨ ===")
            # for i, link in enumerate(remaining_links[:20]):  # åªæ˜¾ç¤ºå‰20ä¸ª
            #     anchor_text = link.get_text().strip()
            #     href = link.get('href', 'N/A')
            #     self.logger.info(f"[{project_id}] å‰©ä½™é“¾æ¥{i+1}: '{anchor_text}' -> {href}")
            # if len(remaining_links) > 20:
            #     self.logger.info(f"[{project_id}] ... è¿˜æœ‰ {len(remaining_links)-20} ä¸ªé“¾æ¥æœªæ˜¾ç¤º")
                    
            links = []
            returned_urls = set()  # é¡µé¢å†…URLå»é‡
            valid_count = 0
            keyword_matched_count = 0
            
            for a_tag in remaining_links:
                href = a_tag['href']
                anchor_text = a_tag.get_text().strip()
                
                # è½¬æ¢ä¸ºç»å¯¹URL
                if not href.startswith(('http://', 'https://')):
                    href = urljoin(response.url, href)
                    
                # ç§»é™¤fragment
                href = href.split('#')[0]
                
                # è·³è¿‡æŒ‡å‘å½“å‰é¡µé¢çš„é“¾æ¥ï¼ˆé¿å…è‡ªå¾ªç¯ï¼‰
                if href == response.url:
                    continue
                    
                if self.is_valid_link(href, response.url):
                    valid_count += 1
                    matched_keyword = self.get_matched_keyword(href, anchor_text) # ä»…åŒ¹é…é”šæ–‡æœ¬
                    if matched_keyword:
                        keyword_matched_count += 1
                        # ä¿å­˜ä¸ºç»“æ„åŒ–å­—å…¸ï¼Œä¾¿äºJSONä¸­æŸ¥çœ‹å’Œç†è§£
                        # é¡µé¢å†…URLå”¯ä¸€æ€§å»é‡
                        if href in returned_urls:
                            continue  # è·³è¿‡é‡å¤ URL
                        returned_urls.add(href)

                        link_info = {
                            "url": href,                    # é“¾æ¥åœ°å€
                            "anchor_text": anchor_text,     # é”šæ–‡æœ¬ï¼ˆé“¾æ¥æ˜¾ç¤ºçš„æ–‡å­—ï¼‰
                            "matched_keyword": matched_keyword  # åŒ¹é…çš„ç™½åå•å…³é”®è¯
                        }
                        links.append(link_info)
                        self.logger.debug(f"[{project_id}] åŒ¹é…é“¾æ¥: {href} (é”šæ–‡æœ¬: '{anchor_text}', å…³é”®è¯: '{matched_keyword}')")

                    else:
                        self.logger.debug(f"[{project_id}] é“¾æ¥ä¸åŒ¹é…å…³é”®è¯: {href} (é”šæ–‡æœ¬: '{anchor_text}')")
                else:
                    self.logger.debug(f"[{project_id}] æ— æ•ˆé“¾æ¥: {href} (é”šæ–‡æœ¬: '{anchor_text}')")
            
            self.logger.info(f"[{project_id}] æœ‰æ•ˆé“¾æ¥æ•°: {valid_count}, å…³é”®è¯åŒ¹é…æ•°: {keyword_matched_count}, æœ€ç»ˆæå–æ•°: {len(links)}")
            return links
            
        except Exception as e:
            project_id = response.meta.get('project_id', 'unknown')
            self.logger.error(f"[{project_id}] é“¾æ¥æå–å¤±è´¥: {e}")
            return []
            
    def is_valid_link(self, url, base_url):
        """æ£€æŸ¥é“¾æ¥æ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed_url = urlparse(url)
            base_parsed = urlparse(base_url)
            
            if not parsed_url.netloc:
                return False
                
            return True
            
        except:
            return False
            
    def get_matched_keyword(self, url, anchor_text):
        """è·å–åŒ¹é…çš„ç™½åå•å…³é”®è¯ï¼›æ£€æŸ¥anchor_textä¸­æ˜¯å¦åŒ…å«ç™½åå•å…³é”®è¯"""
        from ..url_filter import URL_WHITELIST_KEYWORDS
        
        if not anchor_text:
            return None
            
        text_to_check = anchor_text.lower()
        
        for keyword in URL_WHITELIST_KEYWORDS:
            if keyword in text_to_check:
                return keyword
                
        return None
        
    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶çš„ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info("=== çˆ¬è™«å®Œæˆç»Ÿè®¡ ===")
        self.logger.info(f"æ€»é¡¹ç›®æ•°: {self.total_projects}")
        self.logger.info(f"å®Œæˆé¡¹ç›®æ•°: {self.completed_projects}")
        self.logger.info(f"å¤±è´¥é¡¹ç›®æ•°: {self.failed_projects}")
        self.logger.info(f"å…³é—­åŸå› : {reason}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„é¡¹ç›®
        unfinished_projects = []
        for project_id, counter in self.request_counters.items():
            if counter > 0:
                project_name = self.project_data.get(project_id, {}).get('program_name', project_id)
                unfinished_projects.append(f"{project_name} (å‰©ä½™: {counter})")
                
        if unfinished_projects:
            self.logger.warning(f"å‘ç°æœªå®Œæˆçš„é¡¹ç›®: {unfinished_projects}")
            
        # å°è¯•å®Œæˆæ‰€æœ‰å¾…å®Œæˆçš„é¡¹ç›®
        if hasattr(self, '_projects_to_complete') and self._projects_to_complete:
            self.logger.warning(f"å‘ç°å¾…å®Œæˆé¡¹ç›®: {list(self._projects_to_complete)}")
        
        if self.total_projects > 0:
            completion_rate = (self.completed_projects / self.total_projects) * 100
            self.logger.info(f"å®Œæˆç‡: {completion_rate:.1f}%")

    # ------------------------------------------------------------------
    # signal handlers
    # ------------------------------------------------------------------
    def spider_idle(self):
        """å½“çˆ¬è™«å³å°† idle æ—¶ï¼Œå¦‚æœé˜Ÿåˆ—ä¸­è¿˜æœ‰é¡¹ç›®ï¼Œåˆ™å¯åŠ¨ä¸‹ä¸€ä¸ªé¡¹ç›®"""
        if self.project_queue and not self.is_processing_project:
            self.logger.info("spider_idle è§¦å‘ï¼Œè°ƒåº¦ä¸‹ä¸€ä¸ªé¡¹ç›® â€¦")
            responses = list(self.start_next_project())
            requests = []
            
            # åˆ†ç¦»Requestå’ŒItemå¯¹è±¡
            for obj in responses:
                if hasattr(obj, 'url') and hasattr(obj, 'callback'):
                    # è¿™æ˜¯ä¸€ä¸ªRequestå¯¹è±¡
                    requests.append(obj)
                else:
                    # è¿™æ˜¯ä¸€ä¸ªItemå¯¹è±¡ï¼Œç›´æ¥é€šè¿‡pipelineå¤„ç†
                    self.crawler.engine.scraper.itemproc.process_item(obj, self)
            
            if requests:
                for req in requests:
                    # ç›´æ¥é€šè¿‡ engine è°ƒåº¦ï¼Œé¿å…æ·±åº¦å åŠ ï¼ˆScrapy â‰¥2.9 çš„ crawl åªæ¥å— request å‚æ•°ï¼‰
                    self.crawler.engine.crawl(req)
                raise DontCloseSpider  # å‘Šè¯‰ Scrapy æš‚æ—¶ä¸è¦å…³é—­