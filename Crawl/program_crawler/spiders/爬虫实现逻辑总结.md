# 大学项目网页爬虫实现逻辑总结

## 核心功能
这是一个基于Scrapy框架的大学项目网页爬虫，用于抓取12,486个项目的详细信息。

## 主要流程

### 1. 初始化阶段
- 读取CSV文件中的项目URL列表（program_urls.csv或通过参数指定）
- 每个项目包含：项目ID、URL、项目名称、来源文件
- 设置爬取深度限制（最大2层）
- 初始化并发控制和进度跟踪变量

### 2. 顺序爬取控制
- 采用项目队列机制，逐个处理项目
- 每次只启动一个项目的爬取任务
- 使用请求计数器跟踪每个项目的活跃请求数
- 项目完成后自动启动下一个项目

### 3. 页面解析和内容提取

#### HTML内容检查
- 验证响应是否为HTML类型
- 跳过非HTML内容（如PDF、图片等）

#### 文本内容提取
- 提取常规文本元素：段落、标题、列表等
- 特别处理各种表格形式：
  - 传统HTML表格
  - CSS Grid/Flexbox布局
  - 定义列表(dl/dt/dd)
  - Bootstrap行列布局
  - 卡片式布局
- 保持表格结构化格式，添加标识符如[HTML_TABLE]、[GRID_TABLE]等

#### 链接提取和过滤
- 提取页面中所有有效链接及其锚文本
- 跳过导航、页眉、页脚中的链接
- 应用白名单关键词过滤：只爬取包含与硕士项目相关的信息，具体规则见crawl/program_crawler/url_filter.py
- 控制URL路径深度，避免过深链接

### 4. 递归爬取策略
- 根页面（深度0）：提取基本信息和子链接
- 子页面（深度1）：继续提取内容，但不再生成新请求
- 每个有效子链接生成新的Scrapy请求
- 传递项目元数据和匹配关键词信息

### 5. 数据结构
每个项目的数据包含：
- 项目基本信息（ID、名称、根URL、来源文件）
- 爬取时间戳
- 页面列表，每页包含：
  - URL、层级、标题、内容
  - 子链接列表
  - 爬取状态
- 统计信息（总页数、成功/失败页数、成功率）

### 6. 错误处理
- HTTP请求失败通过errback处理
- 内容提取异常使用try-catch捕获
- 记录失败URL和统计信息
- 非HTML内容跳过但记录

### 7. 并发控制
- 全局并发请求数：4
- 单域名并发数：4
- 下载延迟：0.25秒
- 启用自适应节流

### 8. 输出和保存
- 实时输出爬取进度日志。输出到crawl/log
- 对每个爬取的子url，需要显示原anchor text和匹配的keyword
- 每一个项目完成时显示统计信息，然后再进入下一个项目的爬取
- 数据保存为JSON文件，按项目名称命名
- 爬虫关闭时进行最终统计

## 技术特点
1. **智能过滤**：基于白名单关键词的URL过滤机制
2. **结构化提取**：针对各种页面布局的专门处理方法
3. **顺序控制**：避免同时爬取多个项目造成的资源冲突
4. **礼貌爬取**：合理的延迟和并发限制
5. **完整跟踪**：详细的进度监控和错误记录

这个爬虫设计用于大规模、系统性地收集大学项目网站信息，确保数据完整性和爬取效率的平衡。